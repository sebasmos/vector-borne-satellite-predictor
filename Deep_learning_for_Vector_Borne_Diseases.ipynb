{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep learning_for_Vector-Borne_Diseases.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebasmos/vector-borne-satellite-predictor/blob/main/Deep_learning_for_Vector_Borne_Diseases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep learning for Vector-Borne Diseases\n",
        "\n",
        "Complementary open-source collaboration for [MIT Critical data](https://github.com/MITCriticalData-Colombia/Dengue-Prediction-with-Satellite-Images) and [Mimi utily functions](https://github.com/MITCriticalData-Colombia/Dengue-Prediction-with-Satellite-Images)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YX0UIiPv3di3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iYD99ev7Qap"
      },
      "source": [
        "!pip install epiweeks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFIwFlRkHmeC"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KyALAihb4fw"
      },
      "source": [
        "from torchvision.datasets.folder import pil_loader\n",
        "img_folder = 'gdrive/My Drive/dataset_rbg'\n",
        "csv_folder = 'gdrive/My Drive/csv/merge_cases_temperature_WeeklyPrecipitation_timeseries.csv'\n",
        "df = df.append(pd.read_csv(csv_folder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MykBKbD_2-N3"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "sys.path.insert(0,'..')\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import  mean_absolute_error\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "import seaborn as sns\n",
        "from scipy import signal\n",
        "import pickle\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from epiweeks import Week, Year\n",
        "from datetime import date\n",
        "\n",
        "from random import randint, randrange\n",
        "from skimage import io\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "\n",
        "import skimage\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA_Oa1lnw6OV"
      },
      "source": [
        "#torch.cuda.empty_cash()\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJojEXBA3BOK"
      },
      "source": [
        "def get_MAE_score(y_test, y_pred):\n",
        "  # y_test = y_test.detach().numpy()\n",
        "  # y_pred = y_pred.detach().numpy()\n",
        "  # y_test = torch.from_numpy(y_test)\n",
        "  # y_pred = torch.from_numpy(y_pred)\n",
        "  return round(mean_absolute_error(y_test, y_pred), 4)\n",
        "\n",
        "def get_MAPE_score(y_true, y_pred):\n",
        "  \"\"\"Get Mean Absolute Percentage Error (MAPE)\n",
        "  \n",
        "  Calculate the MAPE score based on the prediction. \n",
        "  The lower MAPE socre is, the better the predictions are.\n",
        "\n",
        "  \"\"\"\n",
        "  return round(mean_absolute_percentage_error(y_true, y_pred), 4)\n",
        "def readImg(img_path, resize_ratio=None):\n",
        "  img = io.imread(img_path)\n",
        "\n",
        "  if resize_ratio:\n",
        "    img_rescale = rescale(img, resize_ratio, anti_aliasing=True)\n",
        "\n",
        "  print(os.path.basename(img_path), '(origin shape:', img.shape, '-> rescale:', str(img_rescale.shape) + ')')\n",
        "  return img_rescale\n",
        "\n",
        "\n",
        "# Load data from one of the source\n",
        "def loadData(csv_folder, img_folder, option=None, resize_ratio=None):\n",
        "  if option is None:\n",
        "    # Get data by combining from csv and images\n",
        "    df = loadStructuredData(csv_folder)\n",
        "    info_dict = combineData(img_folder, df, resize_ratio)\n",
        "    \n",
        "    print(len(info_dict['LastDayWeek']), len(info_dict['Image']), len(info_dict['cases_medellin']))\n",
        "\n",
        "  else:\n",
        "    # Load data from previous pickle file\n",
        "    info_dict = 1#loadDataFromPickle(option)\n",
        "  return info_dict\n",
        "  \n",
        "\n",
        "def loadStructuredData(csv_path):\n",
        "  df = pd.DataFrame()\n",
        "  if os.path.isdir(csv_path):\n",
        "    for filename in os.listdir(csv_path):\n",
        "      file_path = os.path.join(csv_path, filename)\n",
        "      df = df.append(pd.read_csv(file_path))\n",
        "  elif os.path.isfile(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "  else:\n",
        "    print('Error: Not folder or file')\n",
        "  return df\n",
        "  \n",
        "def getEpiWeek(origin_str):\n",
        "  \"\"\"Get epi week from string\n",
        "  \"\"\"\n",
        "  date_ls = origin_str.split('-')\n",
        "  return Week.fromdate(date(int(date_ls[0]), int(date_ls[1]), int(date_ls[2])))\n",
        "  \n",
        "def combineData(img_folder, df, resize_ratio=None):\n",
        "  info_dict = {'LastDayWeek':[], 'cases_medellin':[], 'Image':[], 'epi_week':[]}\n",
        "  img_list = os.listdir(img_folder)\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    name = row['LastDayWeek']\n",
        "    week_df = str(getEpiWeek(name))\n",
        "    case = row['cases_medellin']\n",
        "    for img_name in img_list:\n",
        "      \n",
        "      # If image name is image_2017-12-24.tiff -> get 2017-12-24\n",
        "      # Reference Links: https://www.w3schools.com/python/ref_string_join.asp, \n",
        "      #                  https://stackoverflow.com/questions/13174468/how-do-you-join-all-items-in-a-list/13175535\n",
        "      new_img_name = ''.join(i for i in img_name if i.isdigit() or i == '-')      \n",
        "\n",
        "      week_img = str(getEpiWeek(new_img_name))\n",
        "      #print(f\"{week_df} = {week_img}\")\n",
        "      if week_df == week_img:\n",
        "        #print(\"ENTRO\")\n",
        "        img_path = os.path.join(img_folder, img_name)\n",
        "        img = readImg(img_path, resize_ratio)\n",
        "\n",
        "        info_dict['Image'].append(img)\n",
        "        info_dict['LastDayWeek'].append(name)\n",
        "        info_dict['cases_medellin'].append(case)\n",
        "        info_dict['epi_week'].append(week_df)\n",
        "        break\n",
        "\n",
        "  return info_dict\n",
        "\n",
        "def splitTrainTestSet(ratio):\n",
        "  # Split the data into training (ratio) and testing (1 - ratio)\n",
        "  train_val_ratio = ratio\n",
        "  train_num = int(len(info_dict['Image']) * train_val_ratio)\n",
        "\n",
        "  # Change list to array\n",
        "  origin_dimension_X = np.array(info_dict['Image'])\n",
        "  labels = np.array(info_dict['cases_medellin'])\n",
        "\n",
        "  print(''.center(60,'-'))\n",
        "\n",
        "  origin_X_train = origin_dimension_X[:train_num,:,:,:]\n",
        "  y_train = labels[:train_num]\n",
        "  origin_X_test = origin_dimension_X[train_num:,:,:,:]\n",
        "  y_test = labels[train_num:]\n",
        "\n",
        "  # print('Total number of weeks:'.ljust(30), len(origin_dimension_X), 'weeks')\n",
        "  # print('Training input:'.ljust(30), origin_X_train.shape)\n",
        "  # print('Training output:'.ljust(30), y_train.shape)\n",
        "  # print('Testing input:'.ljust(30), origin_X_test.shape)\n",
        "  # print('Testing output:'.ljust(30), y_test.shape) \n",
        "\n",
        "  return origin_X_train, y_train, origin_X_test, y_test\n",
        "\n",
        "# Polynomial Regression\n",
        "def calc_r_2(x, y, degree):\n",
        "    results = {}\n",
        "\n",
        "    coeffs = np.polyfit(x, y, degree)\n",
        "\n",
        "     # Polynomial Coefficients\n",
        "    results['polynomial'] = coeffs.tolist()\n",
        "\n",
        "    # r-squared\n",
        "    p = np.poly1d(coeffs)\n",
        "    # fit values, and mean\n",
        "    yhat = p(x)                         # or [p(z) for z in x]\n",
        "    ybar = np.sum(y)/len(y)          # or sum(y)/len(y)\n",
        "    ssreg = np.sum((yhat-ybar)**2)   # or sum([ (yihat - ybar)**2 for yihat in yhat])\n",
        "    sstot = np.sum((y - ybar)**2)    # or sum([ (yi - ybar)**2 for yi in y])\n",
        "\n",
        "    return ssreg / sstot\n",
        "\n",
        "def classified_with_SVR(origin_X_train, origin_X_test, y_train, y_test):\n",
        "  print('[SVR]'.center(100, '-'))\n",
        "\n",
        "  reshape_X_train = origin_X_train.reshape(origin_X_train.shape[0], -1)\n",
        "  reshape_X_test = origin_X_test.reshape(origin_X_test.shape[0], -1)\n",
        "\n",
        "  regressor = SVR(C=1.0, epsilon=0.2)\n",
        "  regressor.fit(reshape_X_train, y_train)\n",
        "\n",
        "  float_y_pred = regressor.predict(reshape_X_test)\n",
        "  int_y_pred = [int(i) for i in float_y_pred]\n",
        "\n",
        "  print('Predicted')\n",
        "  print(' '.ljust(3, ' '), 'List =', int_y_pred)\n",
        "  print(' '.ljust(3, ' '), 'Mean =', round(np.mean(int_y_pred), 4))\n",
        "  print('')\n",
        "\n",
        "  print('Real')\n",
        "  print(' '.ljust(3, ' '), 'List =', y_test)\n",
        "  print(' '.ljust(3, ' '), 'Mean =', round(np.mean(y_test), 4))\n",
        "  print('')\n",
        "  \n",
        "  MAE = get_MAE_score(y_test, int_y_pred)\n",
        "  MAPE = get_MAPE_score(y_test, int_y_pred)\n",
        "\n",
        "  r_2 = calc_r_2(y_test, int_y_pred, 15)\n",
        "\n",
        "  print('- MAE: ', str(MAE).rjust(8), '(cases different in average)')\n",
        "  print('- MAPE:', str(MAPE).rjust(8), '(times different in average)')\n",
        "  print('- r_squared:', str(r_2).rjust(8), '(times different in average)')\n",
        "\n",
        "  return MAE, MAPE, r_2\n",
        "\n",
        "\n",
        "\n",
        "def dimension_reduct_with_PCA(origin_X_train, origin_X_test, y_train):\n",
        "  print(' PRINCIPAL COMPONENT ANALYSIS  '.center(100, '='))\n",
        "\n",
        "  reshape_X_train = origin_X_train.reshape(origin_X_train.shape[0], -1)\n",
        "  reshape_X_test = origin_X_test.reshape(origin_X_test.shape[0], -1)\n",
        "\n",
        "  pca = PCA(n_components=0.95) \n",
        "  pca_X_train = pca.fit_transform(reshape_X_train)\n",
        "\n",
        "  pca_X_test = pca.transform(reshape_X_test)\n",
        "  print('Origin shape'.ljust(15), reshape_X_train.shape)\n",
        "  print('Resize shape'.ljust(15), pca_X_train.shape)  \n",
        "\n",
        "  return pca_X_train, pca_X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvoQjWxZIpmr",
        "outputId": "2f87049e-8634-42bc-f5f6-88b7aa5b314d"
      },
      "source": [
        "% cd /content/drive/MyDrive/Dengue_prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Dengue_prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBLcjPNmJyZG",
        "outputId": "c8cb432c-5145-418e-ed22-9e42623926a6"
      },
      "source": [
        "from torchvision.datasets.folder import pil_loader\n",
        "\n",
        "dataset_folder = \"/content/drive/MyDrive/Dengue_prediction\"\n",
        "# Change to dataset location here or add shortcut to path \n",
        "# dataset RGB: https://drive.google.com/drive/folders/1f1qWh-Xls8fV4hrv1IhCwsAwnEqYuO-U?usp=sharing\n",
        "image_name = \"./Dengue Research paper/dataset_2015_2018/Sentinel_L1C_2016_2021/dataset_rbg\"\n",
        "img_folder = os.path.join(dataset_folder, image_name)\n",
        "csv_folder = './csv/merge_cases_temperature_WeeklyPrecipitation_timeseries.csv'\n",
        "info_dict = loadData(csv_folder, img_folder, resize_ratio=(0.7, 0.7, 1))\n",
        "\n",
        "print('INFO_DICT'.center(50, '-'))\n",
        "print('keys:', info_dict.keys())\n",
        "print('')\n",
        "\n",
        "print('DENGUE CASES'.center(50, '-'))\n",
        "print('Max weekly dengue cases:', max(info_dict['cases_medellin']))\n",
        "print('Min weekly dengue cases:', min(info_dict['cases_medellin']))\n",
        "print('')\n",
        "\n",
        "print('WEEKS'.center(50, '-'))\n",
        "print('Max week:', max(info_dict['LastDayWeek']))\n",
        "print('Min week:', min(info_dict['LastDayWeek']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_2016-01-03.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-01-10.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-01-17.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-01-24.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-02-07.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-02-14.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-03-06.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-03-20.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-04-03.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-04-17.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-04-24.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-05-01.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-05-08.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-05-15.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-05-22.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-06-05.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-06-12.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-06-19.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-06-26.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-07-03.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-07-10.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-07-17.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-07-24.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-07-31.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-08-07.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-08-14.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-08-21.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-08-28.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-09-04.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-09-11.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-09-18.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-09-25.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-10-02.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-10-09.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-10-16.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-10-23.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-11-06.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-11-13.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-11-20.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-11-27.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-12-04.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-12-11.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2016-12-18.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-01-01.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-01-01.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-01-22.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-01-29.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-02-05.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-02-12.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-02-19.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-02-26.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-03-05.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-03-12.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-03-19.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-03-26.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-04-02.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-04-09.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-04-16.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-04-23.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-04-30.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-05-07.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-05-14.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-05-21.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-05-28.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-06-04.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-06-11.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-06-18.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-06-25.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-07-02.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-07-09.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-07-16.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-07-23.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-07-30.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-08-06.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-08-13.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-08-20.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-08-27.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-09-03.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-09-10.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-09-17.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-09-24.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-10-01.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-10-08.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-10-15.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-10-22.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-10-29.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-11-05.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-11-12.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-11-19.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-11-26.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-12-03.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-12-10.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2017-12-17.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-01-06.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-01-13.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-01-20.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-01-27.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-02-03.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-02-10.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-02-17.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-02-24.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-03-03.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-03-10.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-03-17.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-03-24.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-03-31.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-04-07.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-04-14.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-04-21.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-04-28.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-05-05.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-05-12.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-05-19.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-05-26.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-06-02.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-06-09.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-06-16.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-06-23.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-06-30.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-07-07.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-07-14.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-07-21.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-07-28.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-08-04.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-08-11.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-08-18.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-08-25.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-09-01.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-09-08.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-09-15.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-09-22.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-09-29.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-10-06.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-10-13.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-10-20.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-10-27.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-11-03.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-11-10.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-11-17.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-11-24.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-12-01.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-12-08.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-12-15.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-12-22.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "image_2018-12-29.tiff (origin shape: (1205, 765, 3) -> rescale: (844, 536, 3))\n",
            "145 145 145\n",
            "--------------------INFO_DICT---------------------\n",
            "keys: dict_keys(['LastDayWeek', 'cases_medellin', 'Image', 'epi_week'])\n",
            "\n",
            "-------------------DENGUE CASES-------------------\n",
            "Max weekly dengue cases: 592\n",
            "Min weekly dengue cases: 12\n",
            "\n",
            "----------------------WEEKS-----------------------\n",
            "Max week: 2018-12-29\n",
            "Min week: 2016-01-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqB2y798j9rq"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I87-Jhn4rVo",
        "outputId": "6c281762-3b27-47f5-8984-e6974a32a494"
      },
      "source": [
        "train_val_ratio = 0.8\n",
        "train_num = int(len(info_dict['Image']) * train_val_ratio)\n",
        "\n",
        "  # Change list to array\n",
        "origin_dimension_X = np.array(info_dict['Image'])\n",
        "labels = np.array(info_dict['cases_medellin'])\n",
        "\n",
        "print(''.center(60,'-'))\n",
        "\n",
        "origin_X_train = origin_dimension_X[:train_num,:,:,:]\n",
        "y_train = labels[:train_num]\n",
        "origin_X_test = origin_dimension_X[train_num:,:,:,:]\n",
        "y_test = labels[train_num:]\n",
        "\n",
        "print(f\"origin_X_train: {origin_X_train.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "print(f\"origin_X_train: {origin_X_test.shape}\")\n",
        "print(f\"y_train: {y_test.shape}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "origin_X_train: (116, 844, 536, 3)\n",
            "y_train: (116,)\n",
            "origin_X_train: (29, 844, 536, 3)\n",
            "y_train: (29,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L56HYRkPJKmU",
        "outputId": "1e6e0fd3-ceb9-496d-9171-8db4784587f8"
      },
      "source": [
        "origin_X_train_PCA, origin_X_test_PCA = dimension_reduct_with_PCA(origin_X_train, origin_X_test, y_train)\n",
        "print(f\"origin_X_train: {origin_X_train_PCA.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "print(f\"origin_X_train: {origin_X_test_PCA.shape}\")\n",
        "print(f\"y_train: {y_test.shape}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================== PRINCIPAL COMPONENT ANALYSIS  ===================================\n",
            "Origin shape    (116, 1357152)\n",
            "Resize shape    (116, 50)\n",
            "origin_X_train: (116, 50)\n",
            "y_train: (116,)\n",
            "origin_X_train: (29, 50)\n",
            "y_train: (29,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZQfqqQakn5J"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, data, y):\n",
        "        self.data = data\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    def __getitem__(self, ind):\n",
        "        x = self.data[ind]\n",
        "        y = self.y[ind]\n",
        "        return x, y\n",
        "    \n",
        "class TestDataset(TrainDataset):\n",
        "    def __getitem__(self, ind):\n",
        "        x = self.data[ind]\n",
        "        return x\n",
        "    \n",
        "train_set = TrainDataset(origin_X_train_PCA, y_train)\n",
        "test_set  = TrainDataset(origin_X_test_PCA, y_test)\n",
        "\n",
        "batch_size = 1\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSfrv0QkjUCP"
      },
      "source": [
        "# LSTM with ṔCA features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXOK4vk3Kwl5",
        "outputId": "8b43d397-0d65-4beb-d8e1-5920df3c05ad"
      },
      "source": [
        "input_size = origin_X_train_PCA.shape[1]\n",
        "print(input_size)\n",
        "hidden_size = 230\n",
        "num_layers = 2\n",
        "num_classes = 1\n",
        "output_size = 1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#model.double()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHv2_aezKy-3"
      },
      "source": [
        "\n",
        "class flightLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc_1 = nn.Linear(hidden_size,128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc   = nn.Linear(128, output_size)   \n",
        "        \n",
        "    def forward(self, x, hs, cs):\n",
        "      \n",
        "        out, (hs,cs) = self.lstm(x, (hs,cs)) # out.shape = (batch_size, seq_len, hidden_size)\n",
        "        # output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
        "        out = out.view(-1, self.hidden_size) # out.shape = (seq_len, hidden_size)     \n",
        "        out = self.fc_1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc(out)       \n",
        "        return out\n",
        "\n",
        "x = torch.randn((1, 1000, 50)) # batch - seq length - hidden size\n",
        "hs = torch.zeros(2, x.size(0), hidden_size)\n",
        "cs = torch.zeros(2, x.size(0), hidden_size)\n",
        "model = flightLSTM(input_size, hidden_size, num_layers, num_classes)\n",
        "pred = model(x, hs, cs)\n",
        "print(pred.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD4H3alEdoTx",
        "outputId": "aecc1fd5-074b-4425-ca62-be0cd20e05b3"
      },
      "source": [
        "model = flightLSTM(input_size, hidden_size, num_layers, output_size)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "flightLSTM(\n",
              "  (lstm): LSTM(50, 230, num_layers=2, batch_first=True)\n",
              "  (fc_1): Linear(in_features=230, out_features=128, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u3WR0PcQlHU",
        "outputId": "6ef7e39d-860d-43ce-d6c9-4a213a75a36e"
      },
      "source": [
        "#from model import MPL_model\n",
        "\n",
        "# https://medium.com/deep-learning-study-notes/multi-layer-perceptron-mlp-in-pytorch-21ea46d50e62\n",
        "# cnn https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 100\n",
        "running_mae = 0\n",
        "\n",
        "losses = []\n",
        "\n",
        "v_losses = []\n",
        "# MAE\n",
        "maes = []\n",
        "mae_list = []\n",
        "v_maes = []\n",
        "v_mae_list = []\n",
        "# MAPE\n",
        "mape = []\n",
        "mapes = []\n",
        "mape_val = []\n",
        "mapes_val = []\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    for batch_num, input_data in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        x, y = input_data\n",
        "        x = x.to(device).float()\n",
        "        y = y.to(device)\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        hs = torch.zeros(2, x.size(0), hidden_size).to(device)  \n",
        "        cs = torch.zeros(2, x.size(0), hidden_size).to(device)\n",
        "\n",
        "        output = model(x,hs,cs)\n",
        "        \n",
        "\n",
        "        loss = criterion(output, y.float())\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE\n",
        "        error = torch.abs(output - y).sum().data\n",
        "        maes.append(error)\n",
        "\n",
        "        # MAPE \n",
        "        mape = torch.abs((output-y)/output).sum().data\n",
        "        mapes.append(mape)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, input_data in enumerate(test_loader):\n",
        "            model.eval()\n",
        "            optimizer.zero_grad()\n",
        "            x, y = input_data\n",
        "            x = x.to(device).float()\n",
        "            y = y.to(device)\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "            hs = torch.zeros(2, x.size(0), hidden_size).to(device)  \n",
        "            cs = torch.zeros(2, x.size(0), hidden_size).to(device)\n",
        "\n",
        "            output = model(x,hs,cs)\n",
        "\n",
        "            v_loss = criterion(output, y.float())\n",
        "            \n",
        "            v_losses.append(loss.item())\n",
        "            \n",
        "            \n",
        "            # MAE\n",
        "            v_error = torch.abs(output - y).sum().data\n",
        "            v_maes.append(v_error)\n",
        "\n",
        "            # MAPE \n",
        "            \n",
        "            mape_val = torch.abs((output-y)/output).sum().data\n",
        "            mapes_val.append(mape_val)\n",
        "        \n",
        "    \n",
        "    # MAE\n",
        "    mae_list.append(sum(maes)/len(maes))\n",
        "    v_mae_list.append(sum(v_maes)/len(v_maes))\n",
        "    print('Epoch %d | Loss_training %6.2f | MAE_training %6.2f | MAPE_training %6.2f' % (epoch, sum(losses)/len(losses), sum(maes)/len(maes), \n",
        "                                                                                         sum(mapes)/len(mapes)))\n",
        "    print('Epoch %d |      val_Loss %6.2f |      val_MAE %6.2f |      MAPE_val %6.2f' % (epoch, sum(v_losses)/len(v_losses), sum(v_maes)/len(v_maes),\n",
        "                                                                                         sum(mapes_val)/len(mapes_val)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning:\n",
            "\n",
            "Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss_training 50727.17 | MAE_training 140.66 | MAPE_training 2737.58\n",
            "Epoch 0 |      val_Loss 405.71 |      val_MAE  10.02 |      MAPE_val   0.78\n",
            "Epoch 1 | Loss_training 45357.74 | MAE_training 146.20 | MAPE_training 1370.10\n",
            "Epoch 1 |      val_Loss 434.21 |      val_MAE  20.63 |      MAPE_val   0.79\n",
            "Epoch 2 | Loss_training 40536.96 | MAE_training 139.14 | MAPE_training 924.77\n",
            "Epoch 2 |      val_Loss 309.74 |      val_MAE  24.28 |      MAPE_val   5.14\n",
            "Epoch 3 | Loss_training 36555.35 | MAE_training 132.12 | MAPE_training 694.37\n",
            "Epoch 3 |      val_Loss 616.09 |      val_MAE  29.00 |      MAPE_val   4.09\n",
            "Epoch 4 | Loss_training 32973.08 | MAE_training 123.80 | MAPE_training 555.83\n",
            "Epoch 4 |      val_Loss 550.04 |      val_MAE  34.36 |      MAPE_val   4.13\n",
            "Epoch 5 | Loss_training 29636.18 | MAE_training 114.74 | MAPE_training 463.66\n",
            "Epoch 5 |      val_Loss 500.38 |      val_MAE  40.62 |      MAPE_val   4.03\n",
            "Epoch 6 | Loss_training 26798.88 | MAE_training 106.70 | MAPE_training 397.83\n",
            "Epoch 6 |      val_Loss 436.19 |      val_MAE  46.76 |      MAPE_val   4.45\n",
            "Epoch 7 | Loss_training 24475.87 | MAE_training  99.66 | MAPE_training 348.46\n",
            "Epoch 7 |      val_Loss 390.34 |      val_MAE  51.77 |      MAPE_val   4.32\n",
            "Epoch 8 | Loss_training 22496.61 | MAE_training  93.23 | MAPE_training 309.94\n",
            "Epoch 8 |      val_Loss 350.89 |      val_MAE  55.50 |      MAPE_val   3.92\n",
            "Epoch 9 | Loss_training 20800.67 | MAE_training  87.63 | MAPE_training 279.08\n",
            "Epoch 9 |      val_Loss 316.07 |      val_MAE  57.81 |      MAPE_val   3.60\n",
            "Epoch 10 | Loss_training 19420.76 | MAE_training  82.87 | MAPE_training 253.84\n",
            "Epoch 10 |      val_Loss 288.37 |      val_MAE  60.07 |      MAPE_val   3.40\n",
            "Epoch 11 | Loss_training 18224.31 | MAE_training  78.55 | MAPE_training 232.74\n",
            "Epoch 11 |      val_Loss 266.09 |      val_MAE  62.07 |      MAPE_val   3.17\n",
            "Epoch 12 | Loss_training 17180.38 | MAE_training  74.70 | MAPE_training 214.89\n",
            "Epoch 12 |      val_Loss 245.81 |      val_MAE  63.61 |      MAPE_val   2.97\n",
            "Epoch 13 | Loss_training 16260.18 | MAE_training  71.21 | MAPE_training 199.58\n",
            "Epoch 13 |      val_Loss 228.27 |      val_MAE  64.98 |      MAPE_val   2.81\n",
            "Epoch 14 | Loss_training 15457.33 | MAE_training  68.15 | MAPE_training 186.32\n",
            "Epoch 14 |      val_Loss 213.42 |      val_MAE  66.03 |      MAPE_val   2.67\n",
            "Epoch 15 | Loss_training 14751.67 | MAE_training  65.47 | MAPE_training 174.71\n",
            "Epoch 15 |      val_Loss 201.37 |      val_MAE  66.91 |      MAPE_val   2.54\n",
            "Epoch 16 | Loss_training 14135.32 | MAE_training  63.17 | MAPE_training 164.46\n",
            "Epoch 16 |      val_Loss 191.49 |      val_MAE  67.98 |      MAPE_val   2.47\n",
            "Epoch 17 | Loss_training 13594.73 | MAE_training  61.24 | MAPE_training 155.36\n",
            "Epoch 17 |      val_Loss 180.94 |      val_MAE  69.11 |      MAPE_val   2.37\n",
            "Epoch 18 | Loss_training 13075.75 | MAE_training  59.39 | MAPE_training 147.23\n",
            "Epoch 18 |      val_Loss 171.45 |      val_MAE  69.92 |      MAPE_val   2.28\n",
            "Epoch 19 | Loss_training 12598.89 | MAE_training  57.70 | MAPE_training 139.91\n",
            "Epoch 19 |      val_Loss 163.17 |      val_MAE  70.88 |      MAPE_val   2.21\n",
            "Epoch 20 | Loss_training 12172.55 | MAE_training  56.25 | MAPE_training 133.29\n",
            "Epoch 20 |      val_Loss 156.17 |      val_MAE  71.54 |      MAPE_val   2.13\n",
            "Epoch 21 | Loss_training 11780.78 | MAE_training  54.86 | MAPE_training 127.26\n",
            "Epoch 21 |      val_Loss 149.08 |      val_MAE  72.00 |      MAPE_val   2.06\n",
            "Epoch 22 | Loss_training 11416.75 | MAE_training  53.54 | MAPE_training 121.74\n",
            "Epoch 22 |      val_Loss 145.95 |      val_MAE  72.65 |      MAPE_val   2.00\n",
            "Epoch 23 | Loss_training 11072.52 | MAE_training  52.32 | MAPE_training 116.69\n",
            "Epoch 23 |      val_Loss 143.10 |      val_MAE  73.56 |      MAPE_val   1.94\n",
            "Epoch 24 | Loss_training 10730.85 | MAE_training  51.16 | MAPE_training 112.03\n",
            "Epoch 24 |      val_Loss 137.80 |      val_MAE  74.21 |      MAPE_val   1.89\n",
            "Epoch 25 | Loss_training 10411.05 | MAE_training  49.99 | MAPE_training 107.74\n",
            "Epoch 25 |      val_Loss 135.90 |      val_MAE  74.82 |      MAPE_val   1.85\n",
            "Epoch 26 | Loss_training 10124.82 | MAE_training  48.95 | MAPE_training 103.76\n",
            "Epoch 26 |      val_Loss 130.97 |      val_MAE  75.82 |      MAPE_val   1.81\n",
            "Epoch 27 | Loss_training 9847.76 | MAE_training  48.03 | MAPE_training 100.06\n",
            "Epoch 27 |      val_Loss 128.06 |      val_MAE  76.36 |      MAPE_val   1.79\n",
            "Epoch 28 | Loss_training 9586.77 | MAE_training  47.13 | MAPE_training  96.62\n",
            "Epoch 28 |      val_Loss 123.86 |      val_MAE  76.80 |      MAPE_val   1.75\n",
            "Epoch 29 | Loss_training 9348.83 | MAE_training  46.30 | MAPE_training  93.42\n",
            "Epoch 29 |      val_Loss 120.55 |      val_MAE  77.44 |      MAPE_val   1.72\n",
            "Epoch 30 | Loss_training 9127.97 | MAE_training  45.60 | MAPE_training  90.42\n",
            "Epoch 30 |      val_Loss 116.77 |      val_MAE  78.21 |      MAPE_val   1.69\n",
            "Epoch 31 | Loss_training 8910.12 | MAE_training  44.84 | MAPE_training  87.61\n",
            "Epoch 31 |      val_Loss 113.22 |      val_MAE  78.63 |      MAPE_val   1.66\n",
            "Epoch 32 | Loss_training 8703.44 | MAE_training  44.09 | MAPE_training  84.96\n",
            "Epoch 32 |      val_Loss 109.81 |      val_MAE  79.03 |      MAPE_val   1.63\n",
            "Epoch 33 | Loss_training 8513.77 | MAE_training  43.35 | MAPE_training  82.47\n",
            "Epoch 33 |      val_Loss 106.92 |      val_MAE  79.70 |      MAPE_val   1.61\n",
            "Epoch 34 | Loss_training 8331.22 | MAE_training  42.64 | MAPE_training  80.11\n",
            "Epoch 34 |      val_Loss 104.06 |      val_MAE  80.30 |      MAPE_val   1.59\n",
            "Epoch 35 | Loss_training 8154.33 | MAE_training  41.96 | MAPE_training  77.89\n",
            "Epoch 35 |      val_Loss 101.39 |      val_MAE  80.57 |      MAPE_val   1.57\n",
            "Epoch 36 | Loss_training 7989.88 | MAE_training  41.36 | MAPE_training  75.80\n",
            "Epoch 36 |      val_Loss  99.71 |      val_MAE  80.79 |      MAPE_val   1.55\n",
            "Epoch 37 | Loss_training 7836.22 | MAE_training  40.77 | MAPE_training  73.81\n",
            "Epoch 37 |      val_Loss 101.39 |      val_MAE  81.35 |      MAPE_val   1.53\n",
            "Epoch 38 | Loss_training 7695.77 | MAE_training  40.32 | MAPE_training  71.92\n",
            "Epoch 38 |      val_Loss 100.37 |      val_MAE  82.00 |      MAPE_val   1.51\n",
            "Epoch 39 | Loss_training 7553.85 | MAE_training  39.85 | MAPE_training  70.13\n",
            "Epoch 39 |      val_Loss  99.31 |      val_MAE  82.40 |      MAPE_val   1.49\n",
            "Epoch 40 | Loss_training 7419.85 | MAE_training  39.46 | MAPE_training  68.43\n",
            "Epoch 40 |      val_Loss  99.88 |      val_MAE  82.33 |      MAPE_val   1.47\n",
            "Epoch 41 | Loss_training 7298.27 | MAE_training  39.09 | MAPE_training  66.80\n",
            "Epoch 41 |      val_Loss  98.88 |      val_MAE  82.57 |      MAPE_val   1.46\n",
            "Epoch 42 | Loss_training 7189.40 | MAE_training  38.76 | MAPE_training  65.25\n",
            "Epoch 42 |      val_Loss  96.80 |      val_MAE  83.27 |      MAPE_val   1.44\n",
            "Epoch 43 | Loss_training 7082.32 | MAE_training  38.49 | MAPE_training  63.78\n",
            "Epoch 43 |      val_Loss  95.57 |      val_MAE  83.53 |      MAPE_val   1.42\n",
            "Epoch 44 | Loss_training 6971.08 | MAE_training  38.15 | MAPE_training  62.36\n",
            "Epoch 44 |      val_Loss  93.47 |      val_MAE  83.56 |      MAPE_val   1.41\n",
            "Epoch 45 | Loss_training 6864.91 | MAE_training  37.80 | MAPE_training  61.01\n",
            "Epoch 45 |      val_Loss  91.69 |      val_MAE  83.86 |      MAPE_val   1.39\n",
            "Epoch 46 | Loss_training 6766.19 | MAE_training  37.49 | MAPE_training  59.72\n",
            "Epoch 46 |      val_Loss  92.06 |      val_MAE  84.52 |      MAPE_val   1.38\n",
            "Epoch 47 | Loss_training 6666.40 | MAE_training  37.22 | MAPE_training  58.48\n",
            "Epoch 47 |      val_Loss  92.64 |      val_MAE  84.90 |      MAPE_val   1.36\n",
            "Epoch 48 | Loss_training 6565.47 | MAE_training  36.91 | MAPE_training  57.29\n",
            "Epoch 48 |      val_Loss  91.78 |      val_MAE  85.08 |      MAPE_val   1.35\n",
            "Epoch 49 | Loss_training 6469.32 | MAE_training  36.61 | MAPE_training  56.15\n",
            "Epoch 49 |      val_Loss  90.39 |      val_MAE  85.21 |      MAPE_val   1.34\n",
            "Epoch 50 | Loss_training 6378.03 | MAE_training  36.24 | MAPE_training  55.05\n",
            "Epoch 50 |      val_Loss  88.64 |      val_MAE  85.56 |      MAPE_val   1.32\n",
            "Epoch 51 | Loss_training 6290.92 | MAE_training  35.93 | MAPE_training  54.00\n",
            "Epoch 51 |      val_Loss  87.10 |      val_MAE  86.02 |      MAPE_val   1.31\n",
            "Epoch 52 | Loss_training 6202.21 | MAE_training  35.61 | MAPE_training  52.98\n",
            "Epoch 52 |      val_Loss  85.63 |      val_MAE  86.39 |      MAPE_val   1.30\n",
            "Epoch 53 | Loss_training 6111.21 | MAE_training  35.25 | MAPE_training  52.00\n",
            "Epoch 53 |      val_Loss  84.28 |      val_MAE  86.58 |      MAPE_val   1.29\n",
            "Epoch 54 | Loss_training 6025.40 | MAE_training  34.91 | MAPE_training  51.06\n",
            "Epoch 54 |      val_Loss  82.76 |      val_MAE  86.74 |      MAPE_val   1.28\n",
            "Epoch 55 | Loss_training 5942.50 | MAE_training  34.54 | MAPE_training  50.15\n",
            "Epoch 55 |      val_Loss  81.29 |      val_MAE  87.00 |      MAPE_val   1.27\n",
            "Epoch 56 | Loss_training 5861.96 | MAE_training  34.17 | MAPE_training  49.27\n",
            "Epoch 56 |      val_Loss  79.87 |      val_MAE  87.36 |      MAPE_val   1.26\n",
            "Epoch 57 | Loss_training 5783.15 | MAE_training  33.84 | MAPE_training  48.42\n",
            "Epoch 57 |      val_Loss  78.98 |      val_MAE  87.76 |      MAPE_val   1.25\n",
            "Epoch 58 | Loss_training 5704.75 | MAE_training  33.47 | MAPE_training  47.61\n",
            "Epoch 58 |      val_Loss  77.65 |      val_MAE  88.01 |      MAPE_val   1.24\n",
            "Epoch 59 | Loss_training 5629.79 | MAE_training  33.15 | MAPE_training  46.81\n",
            "Epoch 59 |      val_Loss  76.37 |      val_MAE  88.15 |      MAPE_val   1.23\n",
            "Epoch 60 | Loss_training 5558.39 | MAE_training  32.84 | MAPE_training  46.05\n",
            "Epoch 60 |      val_Loss  75.16 |      val_MAE  88.33 |      MAPE_val   1.22\n",
            "Epoch 61 | Loss_training 5489.62 | MAE_training  32.52 | MAPE_training  45.31\n",
            "Epoch 61 |      val_Loss  73.98 |      val_MAE  88.66 |      MAPE_val   1.22\n",
            "Epoch 62 | Loss_training 5424.30 | MAE_training  32.25 | MAPE_training  44.59\n",
            "Epoch 62 |      val_Loss  75.02 |      val_MAE  89.18 |      MAPE_val   1.22\n",
            "Epoch 63 | Loss_training 5359.92 | MAE_training  32.00 | MAPE_training  43.90\n",
            "Epoch 63 |      val_Loss  74.47 |      val_MAE  89.55 |      MAPE_val   1.21\n",
            "Epoch 64 | Loss_training 5295.46 | MAE_training  31.73 | MAPE_training  43.22\n",
            "Epoch 64 |      val_Loss  73.46 |      val_MAE  89.65 |      MAPE_val   1.20\n",
            "Epoch 65 | Loss_training 5236.94 | MAE_training  31.54 | MAPE_training  42.57\n",
            "Epoch 65 |      val_Loss  72.43 |      val_MAE  89.56 |      MAPE_val   1.19\n",
            "Epoch 66 | Loss_training 5184.21 | MAE_training  31.37 | MAPE_training  41.94\n",
            "Epoch 66 |      val_Loss  71.55 |      val_MAE  89.64 |      MAPE_val   1.18\n",
            "Epoch 67 | Loss_training 5135.07 | MAE_training  31.21 | MAPE_training  41.32\n",
            "Epoch 67 |      val_Loss  70.52 |      val_MAE  90.00 |      MAPE_val   1.19\n",
            "Epoch 68 | Loss_training 5096.20 | MAE_training  31.13 | MAPE_training  40.73\n",
            "Epoch 68 |      val_Loss  69.72 |      val_MAE  90.41 |      MAPE_val   1.18\n",
            "Epoch 69 | Loss_training 5043.27 | MAE_training  30.95 | MAPE_training  40.15\n",
            "Epoch 69 |      val_Loss  68.91 |      val_MAE  90.69 |      MAPE_val   1.17\n",
            "Epoch 70 | Loss_training 4992.84 | MAE_training  30.79 | MAPE_training  39.60\n",
            "Epoch 70 |      val_Loss  68.15 |      val_MAE  90.78 |      MAPE_val   1.16\n",
            "Epoch 71 | Loss_training 4946.62 | MAE_training  30.64 | MAPE_training  39.05\n",
            "Epoch 71 |      val_Loss  67.28 |      val_MAE  91.21 |      MAPE_val   1.16\n",
            "Epoch 72 | Loss_training 4900.16 | MAE_training  30.48 | MAPE_training  38.52\n",
            "Epoch 72 |      val_Loss  66.68 |      val_MAE  91.47 |      MAPE_val   1.15\n",
            "Epoch 73 | Loss_training 4887.15 | MAE_training  30.44 | MAPE_training  38.00\n",
            "Epoch 73 |      val_Loss  65.88 |      val_MAE  91.61 |      MAPE_val   1.14\n",
            "Epoch 74 | Loss_training 4868.80 | MAE_training  30.38 | MAPE_training  37.50\n",
            "Epoch 74 |      val_Loss  65.00 |      val_MAE  91.53 |      MAPE_val   1.14\n",
            "Epoch 75 | Loss_training 4852.67 | MAE_training  30.38 | MAPE_training  37.01\n",
            "Epoch 75 |      val_Loss  64.21 |      val_MAE  91.72 |      MAPE_val   1.13\n",
            "Epoch 76 | Loss_training 4826.50 | MAE_training  30.28 | MAPE_training  36.53\n",
            "Epoch 76 |      val_Loss  63.44 |      val_MAE  92.01 |      MAPE_val   1.12\n",
            "Epoch 77 | Loss_training 4795.70 | MAE_training  30.16 | MAPE_training  36.06\n",
            "Epoch 77 |      val_Loss  62.68 |      val_MAE  92.27 |      MAPE_val   1.12\n",
            "Epoch 78 | Loss_training 4761.85 | MAE_training  30.00 | MAPE_training  35.61\n",
            "Epoch 78 |      val_Loss  61.89 |      val_MAE  92.50 |      MAPE_val   1.11\n",
            "Epoch 79 | Loss_training 4728.43 | MAE_training  29.83 | MAPE_training  35.16\n",
            "Epoch 79 |      val_Loss  61.12 |      val_MAE  92.67 |      MAPE_val   1.11\n",
            "Epoch 80 | Loss_training 4695.76 | MAE_training  29.65 | MAPE_training  34.73\n",
            "Epoch 80 |      val_Loss  60.40 |      val_MAE  92.81 |      MAPE_val   1.10\n",
            "Epoch 81 | Loss_training 4663.80 | MAE_training  29.47 | MAPE_training  34.31\n",
            "Epoch 81 |      val_Loss  59.67 |      val_MAE  92.94 |      MAPE_val   1.09\n",
            "Epoch 82 | Loss_training 4633.43 | MAE_training  29.31 | MAPE_training  33.90\n",
            "Epoch 82 |      val_Loss  58.95 |      val_MAE  93.09 |      MAPE_val   1.09\n",
            "Epoch 83 | Loss_training 4602.91 | MAE_training  29.14 | MAPE_training  33.50\n",
            "Epoch 83 |      val_Loss  58.25 |      val_MAE  93.20 |      MAPE_val   1.08\n",
            "Epoch 84 | Loss_training 4572.37 | MAE_training  28.96 | MAPE_training  33.10\n",
            "Epoch 84 |      val_Loss  57.57 |      val_MAE  93.32 |      MAPE_val   1.08\n",
            "Epoch 85 | Loss_training 4542.28 | MAE_training  28.78 | MAPE_training  32.72\n",
            "Epoch 85 |      val_Loss  56.90 |      val_MAE  93.44 |      MAPE_val   1.07\n",
            "Epoch 86 | Loss_training 4512.95 | MAE_training  28.60 | MAPE_training  32.34\n",
            "Epoch 86 |      val_Loss  56.25 |      val_MAE  93.58 |      MAPE_val   1.07\n",
            "Epoch 87 | Loss_training 4482.78 | MAE_training  28.42 | MAPE_training  31.98\n",
            "Epoch 87 |      val_Loss  55.61 |      val_MAE  93.70 |      MAPE_val   1.06\n",
            "Epoch 88 | Loss_training 4453.21 | MAE_training  28.24 | MAPE_training  31.62\n",
            "Epoch 88 |      val_Loss  54.99 |      val_MAE  93.81 |      MAPE_val   1.06\n",
            "Epoch 89 | Loss_training 4424.41 | MAE_training  28.06 | MAPE_training  31.27\n",
            "Epoch 89 |      val_Loss  54.37 |      val_MAE  93.93 |      MAPE_val   1.05\n",
            "Epoch 90 | Loss_training 4396.23 | MAE_training  27.89 | MAPE_training  30.92\n",
            "Epoch 90 |      val_Loss  53.78 |      val_MAE  94.05 |      MAPE_val   1.05\n",
            "Epoch 91 | Loss_training 4368.24 | MAE_training  27.72 | MAPE_training  30.59\n",
            "Epoch 91 |      val_Loss  53.19 |      val_MAE  94.16 |      MAPE_val   1.05\n",
            "Epoch 92 | Loss_training 4340.54 | MAE_training  27.56 | MAPE_training  30.26\n",
            "Epoch 92 |      val_Loss  52.63 |      val_MAE  94.24 |      MAPE_val   1.04\n",
            "Epoch 93 | Loss_training 4313.42 | MAE_training  27.41 | MAPE_training  29.94\n",
            "Epoch 93 |      val_Loss  52.08 |      val_MAE  94.29 |      MAPE_val   1.04\n",
            "Epoch 94 | Loss_training 4287.02 | MAE_training  27.26 | MAPE_training  29.63\n",
            "Epoch 94 |      val_Loss  51.56 |      val_MAE  94.39 |      MAPE_val   1.03\n",
            "Epoch 95 | Loss_training 4262.54 | MAE_training  27.13 | MAPE_training  29.32\n",
            "Epoch 95 |      val_Loss  51.13 |      val_MAE  94.52 |      MAPE_val   1.03\n",
            "Epoch 96 | Loss_training 4237.11 | MAE_training  27.01 | MAPE_training  29.02\n",
            "Epoch 96 |      val_Loss  50.84 |      val_MAE  94.63 |      MAPE_val   1.03\n",
            "Epoch 97 | Loss_training 4210.89 | MAE_training  26.88 | MAPE_training  28.72\n",
            "Epoch 97 |      val_Loss  50.51 |      val_MAE  94.62 |      MAPE_val   1.02\n",
            "Epoch 98 | Loss_training 4185.57 | MAE_training  26.77 | MAPE_training  28.43\n",
            "Epoch 98 |      val_Loss  50.01 |      val_MAE  94.55 |      MAPE_val   1.02\n",
            "Epoch 99 | Loss_training 4160.57 | MAE_training  26.67 | MAPE_training  28.15\n",
            "Epoch 99 |      val_Loss  49.54 |      val_MAE  94.66 |      MAPE_val   1.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "OuUwyXTbSKr_",
        "outputId": "6fd7c276-2e66-421d-e9d6-4811a31b3d35"
      },
      "source": [
        "epochs = range(0,100)\n",
        "plt.figure(figsize=[8., 6.])\n",
        "plt.plot(epochs, mae_list,  label='MAE')\n",
        "plt.plot(epochs, v_mae_list, label='Val_MAE')\n",
        "plt.title('Training MAE vs epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ASyrWEIe7x1"
      },
      "source": [
        "# GRU - RNN with PCA - shortened"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmpC4tTpk5VW",
        "outputId": "7574b156-4776-4757-d0d2-1378d505af2c"
      },
      "source": [
        "# https://towardsdatascience.com/lstms-in-pytorch-528b0440244\n",
        "input_size = origin_X_train_PCA.shape[1] # The number of variables in your sequence data. \n",
        "print(input_size)\n",
        "hidden_size = 100 # The number of hidden nodes in the LSTM layer.\n",
        "sequence_length = 1 # its 1, but it it was an image, then this consideres the width of the image i.e. X.shape[2] as we mind the width\n",
        "num_layers = 2\n",
        "num_classes = 1\n",
        "output_size = 1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkzWw4v8quFD"
      },
      "source": [
        "class RNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc_1 = nn.Linear(hidden_size,1024)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc_1_1 = nn.Linear(1024,512)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc   = nn.Linear(512, output_size)   \n",
        "        #self.final = torch.log_softmax(output_size, dim=1)\n",
        "        \n",
        "    def forward(self, x, hs, cs):\n",
        "      \n",
        "        out, (hs,cs) = self.lstm(x, (hs,cs)) # out.shape = (batch_size, seq_len, hidden_size)\n",
        "        # output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
        "        out = out.view(-1, self.hidden_size) # out.shape = (seq_len, hidden_size)     \n",
        "        out = self.fc_1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.fc_1_1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc(out)       \n",
        "        return out\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = out.reshape(out.shape[0], -1)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "        \n",
        "# Recurrent neural network with GRU (many-to-one)\n",
        "class RNN_GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN_GRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = out.reshape(out.shape[0], -1)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsZHgCnmgLAh",
        "outputId": "362a3b04-9b46-4df9-f110-ac903e56d4e4"
      },
      "source": [
        "model = RNN_GRU(input_size, hidden_size, num_layers, output_size)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN_GRU(\n",
              "  (gru): GRU(50, 100, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0BwYisee7On",
        "outputId": "1fb5c1fc-1728-4bcb-d53c-4fc16f0ab357"
      },
      "source": [
        "#from model import MPL_model\n",
        "\n",
        "# https://medium.com/deep-learning-study-notes/multi-layer-perceptron-mlp-in-pytorch-21ea46d50e62\n",
        "# cnn https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 100\n",
        "running_mae = 0\n",
        "\n",
        "losses = []\n",
        "\n",
        "v_losses = []\n",
        "# MAE\n",
        "maes = []\n",
        "mae_list = []\n",
        "v_maes = []\n",
        "v_mae_list = []\n",
        "# MAPE\n",
        "mape = []\n",
        "mapes = []\n",
        "mapes_list = []\n",
        "mape_val = []\n",
        "mapes_val = []\n",
        "mapes_val_list = []\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer):\n",
        "\n",
        "    for batch_num, input_data in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        x, y = input_data\n",
        "        x = x.to(device).float()\n",
        "        y = y.to(device)\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        hs = torch.zeros(2, x.size(0), hidden_size).to(device)  \n",
        "        cs = torch.zeros(2, x.size(0), hidden_size).to(device)\n",
        "\n",
        "        #output = model(x,hs,cs)\n",
        "        output = model(x)\n",
        "\n",
        "        loss = criterion(output, y.float())\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE\n",
        "        error = torch.abs(output - y).sum().data\n",
        "        maes.append(error)\n",
        "\n",
        "        # MAPE \n",
        "        mape = torch.abs((output-y)/output).sum().data\n",
        "        mapes.append(mape)\n",
        "    \n",
        "    return maes, mapes\n",
        "\n",
        "def test(train_loader, model, criterion, optimizer):\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_num, input_data in enumerate(test_loader):\n",
        "          model.eval()\n",
        "          optimizer.zero_grad()\n",
        "          x, y = input_data\n",
        "          x = x.to(device).float()\n",
        "          y = y.to(device)\n",
        "          x = x.unsqueeze(0)\n",
        "\n",
        "          hs = torch.zeros(2, x.size(0), hidden_size).to(device)  \n",
        "          cs = torch.zeros(2, x.size(0), hidden_size).to(device)\n",
        "\n",
        "          #output = model(x,hs,cs)\n",
        "          output = model(x)\n",
        "          v_loss = criterion(output, y.float())\n",
        "          \n",
        "          v_losses.append(loss.item())\n",
        "          \n",
        "          \n",
        "          # MAE\n",
        "          v_error = torch.abs(output - y).sum().data\n",
        "          v_maes.append(v_error)\n",
        "\n",
        "          # MAPE \n",
        "          \n",
        "          mape_val = torch.abs((output-y)/output).sum().data\n",
        "          mapes_val.append(mape_val)\n",
        "      return v_maes, mapes_val\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "\n",
        "    maes, mapes = train(train_loader, model, criterion, optimizer)\n",
        "\n",
        "    v_maes, mapes_val = test(test_loader, model, criterion, optimizer)\n",
        "    \n",
        "    # MAE\n",
        "\n",
        "    mae_list.append(sum(maes)/len(maes))\n",
        "    v_mae_list.append(sum(v_maes)/len(v_maes))\n",
        "\n",
        "    mapes_list.append(sum(mapes)/len(mapes))\n",
        "    mapes_val_list.append(sum(mapes_val)/len(mapes_val))\n",
        "\n",
        "    print('Epoch %d | Loss_training %6.2f | MAE_training %6.2f | MAPE_training %6.2f' % (epoch, sum(losses)/len(losses), sum(maes)/len(maes), \n",
        "                                                                                         sum(mapes)/len(mapes)))\n",
        "    print('Epoch %d | val_Loss %6.2f | val_MAE %6.2f | MAPE_val %6.2f' % (epoch, sum(v_losses)/len(v_losses), sum(v_maes)/len(v_maes),\n",
        "                                                                                         sum(mapes_val)/len(mapes_val)))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning:\n",
            "\n",
            "Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss_training 50036.56 | MAE_training 142.86 | MAPE_training  57.18\n",
            "Epoch 0 | val_Loss 2593.94 | val_MAE  16.20 | MAPE_val   5.01\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Epoch 1 | Loss_training 49036.79 | MAE_training 138.73 | MAPE_training  35.54\n",
            "Epoch 1 | val_Loss 2593.94 | val_MAE  12.35 | MAPE_val   3.73\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Epoch 2 | Loss_training 47856.60 | MAE_training 134.80 | MAPE_training  25.79\n",
            "Epoch 2 | val_Loss 2593.94 | val_MAE  10.52 | MAPE_val   8.12\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Epoch 3 | Loss_training 46743.76 | MAE_training 131.76 | MAPE_training  20.40\n",
            "Epoch 3 | val_Loss 2593.94 | val_MAE  10.66 | MAPE_val   6.97\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Epoch 4 | Loss_training 45730.44 | MAE_training 129.51 | MAPE_training  16.98\n",
            "Epoch 4 | val_Loss 2593.94 | val_MAE  11.80 | MAPE_val   5.94\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Epoch 5 | Loss_training 44803.75 | MAE_training 127.75 | MAPE_training  14.61\n",
            "Epoch 5 | val_Loss 2593.94 | val_MAE  13.32 | MAPE_val   5.09\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Epoch 6 | Loss_training 43949.12 | MAE_training 126.34 | MAPE_training  12.87\n",
            "Epoch 6 | val_Loss 2593.94 | val_MAE  14.98 | MAPE_val   4.53\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Epoch 7 | Loss_training 43150.12 | MAE_training 125.16 | MAPE_training  11.54\n",
            "Epoch 7 | val_Loss 2593.94 | val_MAE  16.54 | MAPE_val   4.09\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Epoch 8 | Loss_training 42397.29 | MAE_training 124.05 | MAPE_training  10.47\n",
            "Epoch 8 | val_Loss 2593.94 | val_MAE  18.07 | MAPE_val   3.75\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Epoch 9 | Loss_training 41682.53 | MAE_training 123.01 | MAPE_training   9.59\n",
            "Epoch 9 | val_Loss 2593.94 | val_MAE  19.56 | MAPE_val   3.51\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Epoch 10 | Loss_training 41001.31 | MAE_training 122.05 | MAPE_training   8.87\n",
            "Epoch 10 | val_Loss 2593.94 | val_MAE  21.09 | MAPE_val   3.28\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Epoch 11 | Loss_training 40346.88 | MAE_training 121.09 | MAPE_training   8.26\n",
            "Epoch 11 | val_Loss 2593.94 | val_MAE  22.18 | MAPE_val   3.10\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Epoch 12 | Loss_training 39726.80 | MAE_training 120.22 | MAPE_training   7.73\n",
            "Epoch 12 | val_Loss 2593.94 | val_MAE  23.59 | MAPE_val   2.93\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Epoch 13 | Loss_training 39120.23 | MAE_training 119.31 | MAPE_training   7.29\n",
            "Epoch 13 | val_Loss 2593.94 | val_MAE  24.56 | MAPE_val   2.79\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Epoch 14 | Loss_training 38532.64 | MAE_training 118.33 | MAPE_training   6.89\n",
            "Epoch 14 | val_Loss 2593.94 | val_MAE  25.44 | MAPE_val   2.68\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Epoch 15 | Loss_training 37965.76 | MAE_training 117.40 | MAPE_training   6.53\n",
            "Epoch 15 | val_Loss 2593.94 | val_MAE  26.38 | MAPE_val   2.58\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Epoch 16 | Loss_training 37407.09 | MAE_training 116.35 | MAPE_training   6.21\n",
            "Epoch 16 | val_Loss 2593.94 | val_MAE  27.18 | MAPE_val   2.49\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Epoch 17 | Loss_training 36878.28 | MAE_training 115.39 | MAPE_training   5.92\n",
            "Epoch 17 | val_Loss 2593.94 | val_MAE  28.23 | MAPE_val   2.40\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Epoch 18 | Loss_training 36352.83 | MAE_training 114.44 | MAPE_training   5.66\n",
            "Epoch 18 | val_Loss 2593.94 | val_MAE  29.09 | MAPE_val   2.36\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Epoch 19 | Loss_training 35841.41 | MAE_training 113.48 | MAPE_training   5.43\n",
            "Epoch 19 | val_Loss 2593.94 | val_MAE  30.00 | MAPE_val   2.32\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Epoch 20 | Loss_training 35341.90 | MAE_training 112.50 | MAPE_training   5.21\n",
            "Epoch 20 | val_Loss 2593.94 | val_MAE  30.70 | MAPE_val   2.26\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Epoch 21 | Loss_training 34860.97 | MAE_training 111.56 | MAPE_training   5.01\n",
            "Epoch 21 | val_Loss 2593.94 | val_MAE  31.42 | MAPE_val   2.21\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Epoch 22 | Loss_training 34380.31 | MAE_training 110.60 | MAPE_training   4.83\n",
            "Epoch 22 | val_Loss 2593.94 | val_MAE  32.00 | MAPE_val   2.16\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Epoch 23 | Loss_training 33936.67 | MAE_training 109.77 | MAPE_training   4.66\n",
            "Epoch 23 | val_Loss 2593.94 | val_MAE  32.90 | MAPE_val   2.13\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Epoch 24 | Loss_training 33478.39 | MAE_training 108.86 | MAPE_training   4.51\n",
            "Epoch 24 | val_Loss 2593.94 | val_MAE  33.69 | MAPE_val   2.23\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Epoch 25 | Loss_training 33032.05 | MAE_training 107.95 | MAPE_training   4.36\n",
            "Epoch 25 | val_Loss 2593.94 | val_MAE  34.53 | MAPE_val   2.19\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Epoch 26 | Loss_training 32587.47 | MAE_training 106.99 | MAPE_training   4.22\n",
            "Epoch 26 | val_Loss 2593.94 | val_MAE  35.33 | MAPE_val   2.22\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Epoch 27 | Loss_training 32147.63 | MAE_training 106.02 | MAPE_training   4.09\n",
            "Epoch 27 | val_Loss 2593.94 | val_MAE  36.14 | MAPE_val   2.22\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Epoch 28 | Loss_training 31718.29 | MAE_training 105.06 | MAPE_training   3.97\n",
            "Epoch 28 | val_Loss 2593.94 | val_MAE  36.94 | MAPE_val   2.31\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Epoch 29 | Loss_training 31295.91 | MAE_training 104.09 | MAPE_training   3.86\n",
            "Epoch 29 | val_Loss 2593.94 | val_MAE  37.72 | MAPE_val   2.28\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Epoch 30 | Loss_training 30881.43 | MAE_training 103.13 | MAPE_training   3.75\n",
            "Epoch 30 | val_Loss 2593.94 | val_MAE  38.47 | MAPE_val   2.34\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Epoch 31 | Loss_training 30474.95 | MAE_training 102.17 | MAPE_training   3.65\n",
            "Epoch 31 | val_Loss 2593.94 | val_MAE  39.24 | MAPE_val   2.73\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Epoch 32 | Loss_training 30076.55 | MAE_training 101.22 | MAPE_training   3.55\n",
            "Epoch 32 | val_Loss 2593.94 | val_MAE  40.04 | MAPE_val   2.70\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Epoch 33 | Loss_training 29686.01 | MAE_training 100.29 | MAPE_training   3.46\n",
            "Epoch 33 | val_Loss 2593.94 | val_MAE  40.79 | MAPE_val   2.66\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Epoch 34 | Loss_training 29306.79 | MAE_training  99.44 | MAPE_training   3.38\n",
            "Epoch 34 | val_Loss 2593.94 | val_MAE  41.51 | MAPE_val   2.93\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Epoch 35 | Loss_training 28935.00 | MAE_training  98.58 | MAPE_training   3.29\n",
            "Epoch 35 | val_Loss 2593.94 | val_MAE  42.29 | MAPE_val   3.16\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Epoch 36 | Loss_training 28570.18 | MAE_training  97.73 | MAPE_training   3.22\n",
            "Epoch 36 | val_Loss 2593.94 | val_MAE  43.07 | MAPE_val   3.13\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Epoch 37 | Loss_training 28212.09 | MAE_training  96.87 | MAPE_training   3.14\n",
            "Epoch 37 | val_Loss 2593.94 | val_MAE  43.84 | MAPE_val   3.08\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Epoch 38 | Loss_training 27861.41 | MAE_training  96.03 | MAPE_training   3.07\n",
            "Epoch 38 | val_Loss 2593.94 | val_MAE  44.60 | MAPE_val   3.03\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Epoch 39 | Loss_training 27518.29 | MAE_training  95.20 | MAPE_training   3.00\n",
            "Epoch 39 | val_Loss 2593.94 | val_MAE  45.36 | MAPE_val   2.98\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Epoch 40 | Loss_training 27183.09 | MAE_training  94.38 | MAPE_training   2.94\n",
            "Epoch 40 | val_Loss 2593.94 | val_MAE  46.14 | MAPE_val   2.93\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Epoch 41 | Loss_training 26854.50 | MAE_training  93.58 | MAPE_training   2.88\n",
            "Epoch 41 | val_Loss 2593.94 | val_MAE  46.89 | MAPE_val   2.89\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Epoch 42 | Loss_training 26532.98 | MAE_training  92.80 | MAPE_training   2.82\n",
            "Epoch 42 | val_Loss 2593.94 | val_MAE  47.61 | MAPE_val   2.84\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Epoch 43 | Loss_training 26218.42 | MAE_training  92.03 | MAPE_training   2.76\n",
            "Epoch 43 | val_Loss 2593.94 | val_MAE  48.32 | MAPE_val   2.81\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Epoch 44 | Loss_training 25910.12 | MAE_training  91.27 | MAPE_training   2.71\n",
            "Epoch 44 | val_Loss 2593.94 | val_MAE  49.02 | MAPE_val   2.77\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Epoch 45 | Loss_training 25608.47 | MAE_training  90.52 | MAPE_training   2.65\n",
            "Epoch 45 | val_Loss 2593.94 | val_MAE  49.73 | MAPE_val   2.73\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Epoch 46 | Loss_training 25312.36 | MAE_training  89.78 | MAPE_training   2.61\n",
            "Epoch 46 | val_Loss 2593.94 | val_MAE  50.43 | MAPE_val   2.70\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Epoch 47 | Loss_training 25022.77 | MAE_training  89.04 | MAPE_training   2.57\n",
            "Epoch 47 | val_Loss 2593.94 | val_MAE  51.09 | MAPE_val   2.67\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Epoch 48 | Loss_training 24738.63 | MAE_training  88.32 | MAPE_training   2.52\n",
            "Epoch 48 | val_Loss 2593.94 | val_MAE  51.67 | MAPE_val   2.64\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Epoch 49 | Loss_training 24461.76 | MAE_training  87.62 | MAPE_training   2.48\n",
            "Epoch 49 | val_Loss 2593.94 | val_MAE  52.26 | MAPE_val   2.62\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Epoch 50 | Loss_training 24188.73 | MAE_training  86.92 | MAPE_training   2.43\n",
            "Epoch 50 | val_Loss 2593.94 | val_MAE  52.85 | MAPE_val   2.58\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Epoch 51 | Loss_training 23921.35 | MAE_training  86.22 | MAPE_training   2.39\n",
            "Epoch 51 | val_Loss 2593.94 | val_MAE  53.42 | MAPE_val   2.56\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Epoch 52 | Loss_training 23657.81 | MAE_training  85.53 | MAPE_training   2.35\n",
            "Epoch 52 | val_Loss 2593.94 | val_MAE  53.97 | MAPE_val   2.53\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Epoch 53 | Loss_training 23399.06 | MAE_training  84.86 | MAPE_training   2.31\n",
            "Epoch 53 | val_Loss 2593.94 | val_MAE  54.49 | MAPE_val   2.50\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Epoch 54 | Loss_training 23147.43 | MAE_training  84.21 | MAPE_training   2.27\n",
            "Epoch 54 | val_Loss 2593.94 | val_MAE  54.97 | MAPE_val   3.51\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Epoch 55 | Loss_training 22899.70 | MAE_training  83.61 | MAPE_training   2.24\n",
            "Epoch 55 | val_Loss 2593.94 | val_MAE  55.48 | MAPE_val   3.49\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Epoch 56 | Loss_training 22661.21 | MAE_training  83.02 | MAPE_training   2.21\n",
            "Epoch 56 | val_Loss 2593.94 | val_MAE  55.98 | MAPE_val   3.45\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Epoch 57 | Loss_training 22427.21 | MAE_training  82.45 | MAPE_training   2.18\n",
            "Epoch 57 | val_Loss 2593.94 | val_MAE  56.49 | MAPE_val   3.40\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Epoch 58 | Loss_training 22192.90 | MAE_training  81.88 | MAPE_training   2.14\n",
            "Epoch 58 | val_Loss 2593.94 | val_MAE  57.00 | MAPE_val   3.36\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Epoch 59 | Loss_training 21963.98 | MAE_training  81.32 | MAPE_training   2.11\n",
            "Epoch 59 | val_Loss 2593.94 | val_MAE  57.53 | MAPE_val   3.32\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Epoch 60 | Loss_training 21737.90 | MAE_training  80.74 | MAPE_training   2.08\n",
            "Epoch 60 | val_Loss 2593.94 | val_MAE  58.07 | MAPE_val   3.29\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Epoch 61 | Loss_training 21515.66 | MAE_training  80.22 | MAPE_training   2.05\n",
            "Epoch 61 | val_Loss 2593.94 | val_MAE  58.54 | MAPE_val   3.25\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Epoch 62 | Loss_training 21303.42 | MAE_training  79.75 | MAPE_training   2.03\n",
            "Epoch 62 | val_Loss 2593.94 | val_MAE  58.94 | MAPE_val   3.21\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Epoch 63 | Loss_training 21093.00 | MAE_training  79.30 | MAPE_training   2.00\n",
            "Epoch 63 | val_Loss 2593.94 | val_MAE  59.35 | MAPE_val   3.19\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Epoch 64 | Loss_training 20881.62 | MAE_training  78.80 | MAPE_training   1.98\n",
            "Epoch 64 | val_Loss 2593.94 | val_MAE  59.83 | MAPE_val   3.15\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Epoch 65 | Loss_training 20676.17 | MAE_training  78.30 | MAPE_training   1.95\n",
            "Epoch 65 | val_Loss 2593.94 | val_MAE  60.36 | MAPE_val   3.13\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Epoch 66 | Loss_training 20475.66 | MAE_training  77.83 | MAPE_training   1.93\n",
            "Epoch 66 | val_Loss 2593.94 | val_MAE  60.82 | MAPE_val   3.10\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Epoch 67 | Loss_training 20273.78 | MAE_training  77.33 | MAPE_training   1.91\n",
            "Epoch 67 | val_Loss 2593.94 | val_MAE  61.36 | MAPE_val   3.07\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Epoch 68 | Loss_training 20071.80 | MAE_training  76.80 | MAPE_training   1.88\n",
            "Epoch 68 | val_Loss 2593.94 | val_MAE  61.95 | MAPE_val   3.04\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Epoch 69 | Loss_training 19874.06 | MAE_training  76.29 | MAPE_training   1.86\n",
            "Epoch 69 | val_Loss 2593.94 | val_MAE  62.56 | MAPE_val   3.02\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Epoch 70 | Loss_training 19679.67 | MAE_training  75.78 | MAPE_training   1.86\n",
            "Epoch 70 | val_Loss 2593.94 | val_MAE  63.15 | MAPE_val   3.01\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Epoch 71 | Loss_training 19488.92 | MAE_training  75.28 | MAPE_training   1.84\n",
            "Epoch 71 | val_Loss 2593.94 | val_MAE  63.77 | MAPE_val   2.99\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Epoch 72 | Loss_training 19298.26 | MAE_training  74.77 | MAPE_training   1.81\n",
            "Epoch 72 | val_Loss 2593.94 | val_MAE  64.38 | MAPE_val   2.96\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Epoch 73 | Loss_training 19109.63 | MAE_training  74.25 | MAPE_training   1.79\n",
            "Epoch 73 | val_Loss 2593.94 | val_MAE  64.98 | MAPE_val   2.94\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Epoch 74 | Loss_training 18923.69 | MAE_training  73.72 | MAPE_training   1.77\n",
            "Epoch 74 | val_Loss 2593.94 | val_MAE  65.57 | MAPE_val   2.91\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Epoch 75 | Loss_training 18740.50 | MAE_training  73.20 | MAPE_training   1.75\n",
            "Epoch 75 | val_Loss 2593.94 | val_MAE  66.14 | MAPE_val   2.89\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Epoch 76 | Loss_training 18560.08 | MAE_training  72.68 | MAPE_training   1.73\n",
            "Epoch 76 | val_Loss 2593.94 | val_MAE  66.71 | MAPE_val   2.87\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Epoch 77 | Loss_training 18382.33 | MAE_training  72.16 | MAPE_training   1.71\n",
            "Epoch 77 | val_Loss 2593.94 | val_MAE  67.26 | MAPE_val   2.85\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Epoch 78 | Loss_training 18207.31 | MAE_training  71.65 | MAPE_training   1.69\n",
            "Epoch 78 | val_Loss 2593.94 | val_MAE  67.82 | MAPE_val   2.82\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Epoch 79 | Loss_training 18034.98 | MAE_training  71.14 | MAPE_training   1.67\n",
            "Epoch 79 | val_Loss 2593.94 | val_MAE  68.33 | MAPE_val   2.80\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Epoch 80 | Loss_training 17865.40 | MAE_training  70.65 | MAPE_training   1.65\n",
            "Epoch 80 | val_Loss 2593.94 | val_MAE  68.82 | MAPE_val   2.78\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Epoch 81 | Loss_training 17698.36 | MAE_training  70.16 | MAPE_training   1.63\n",
            "Epoch 81 | val_Loss 2593.94 | val_MAE  69.31 | MAPE_val   2.76\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Epoch 82 | Loss_training 17533.43 | MAE_training  69.69 | MAPE_training   1.61\n",
            "Epoch 82 | val_Loss 2593.94 | val_MAE  69.79 | MAPE_val   2.74\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Epoch 83 | Loss_training 17371.98 | MAE_training  69.22 | MAPE_training   1.60\n",
            "Epoch 83 | val_Loss 2593.94 | val_MAE  70.24 | MAPE_val   2.72\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Epoch 84 | Loss_training 17212.63 | MAE_training  68.75 | MAPE_training   1.58\n",
            "Epoch 84 | val_Loss 2593.94 | val_MAE  70.69 | MAPE_val   2.71\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Epoch 85 | Loss_training 17055.37 | MAE_training  68.29 | MAPE_training   1.56\n",
            "Epoch 85 | val_Loss 2593.94 | val_MAE  71.11 | MAPE_val   2.69\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Epoch 86 | Loss_training 16900.47 | MAE_training  67.83 | MAPE_training   1.55\n",
            "Epoch 86 | val_Loss 2593.94 | val_MAE  71.52 | MAPE_val   2.99\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Epoch 87 | Loss_training 16748.32 | MAE_training  67.38 | MAPE_training   1.53\n",
            "Epoch 87 | val_Loss 2593.94 | val_MAE  71.93 | MAPE_val   2.98\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Epoch 88 | Loss_training 16597.96 | MAE_training  66.94 | MAPE_training   1.52\n",
            "Epoch 88 | val_Loss 2593.94 | val_MAE  72.33 | MAPE_val   2.95\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Epoch 89 | Loss_training 16449.90 | MAE_training  66.51 | MAPE_training   1.50\n",
            "Epoch 89 | val_Loss 2593.94 | val_MAE  72.73 | MAPE_val   2.94\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Epoch 90 | Loss_training 16304.75 | MAE_training  66.10 | MAPE_training   1.49\n",
            "Epoch 90 | val_Loss 2593.94 | val_MAE  73.07 | MAPE_val   2.93\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Epoch 91 | Loss_training 16162.03 | MAE_training  65.69 | MAPE_training   1.47\n",
            "Epoch 91 | val_Loss 2593.94 | val_MAE  73.40 | MAPE_val   2.98\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Epoch 92 | Loss_training 16020.97 | MAE_training  65.29 | MAPE_training   1.46\n",
            "Epoch 92 | val_Loss 2593.94 | val_MAE  73.74 | MAPE_val   2.97\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Epoch 93 | Loss_training 15882.69 | MAE_training  64.91 | MAPE_training   1.45\n",
            "Epoch 93 | val_Loss 2593.94 | val_MAE  74.14 | MAPE_val   2.96\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Epoch 94 | Loss_training 15745.80 | MAE_training  64.51 | MAPE_training   1.44\n",
            "Epoch 94 | val_Loss 2593.94 | val_MAE  74.52 | MAPE_val   2.95\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Epoch 95 | Loss_training 15610.85 | MAE_training  64.11 | MAPE_training   1.43\n",
            "Epoch 95 | val_Loss 2593.94 | val_MAE  74.93 | MAPE_val   2.94\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Epoch 96 | Loss_training 15477.61 | MAE_training  63.70 | MAPE_training   1.42\n",
            "Epoch 96 | val_Loss 2593.94 | val_MAE  75.33 | MAPE_val   2.92\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Epoch 97 | Loss_training 15346.47 | MAE_training  63.29 | MAPE_training   1.40\n",
            "Epoch 97 | val_Loss 2593.94 | val_MAE  75.71 | MAPE_val   2.92\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Epoch 98 | Loss_training 15216.99 | MAE_training  62.89 | MAPE_training   1.39\n",
            "Epoch 98 | val_Loss 2593.94 | val_MAE  76.07 | MAPE_val   2.91\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Epoch 99 | Loss_training 15089.50 | MAE_training  62.48 | MAPE_training   1.38\n",
            "Epoch 99 | val_Loss 2593.94 | val_MAE  76.42 | MAPE_val   2.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQrQjjuafTWs",
        "outputId": "67d7136f-0bd6-41e5-866e-9cd186d00675"
      },
      "source": [
        "epochs = range(0,100)\n",
        "plt.figure(figsize=[8., 6.])\n",
        "plt.plot(epochs, mae_list,  label='MAE')\n",
        "plt.plot(epochs, v_mae_list, label='Val_MAE')\n",
        "plt.title('Training MAE vs epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn--ICeDzlfn"
      },
      "source": [
        "# Feature extraction off-the-shelf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGy3SMDV0bH2",
        "outputId": "558a060b-27fd-4518-e81e-111ce17f1c8f"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from torchvision.datasets.utils import download_file_from_google_drive\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G93KpfZm29I1"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "      transforms.ToTensor()               \n",
        "])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    #transforms.Resize(224),    \n",
        "    #transforms.CenterCrop(192),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))                  \n",
        "])\n",
        "\n",
        "class TrainDataset_improved(Dataset):\n",
        "    def __init__(self, data, y, transform = None):\n",
        "        self.data = data\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    def __getitem__(self, ind):\n",
        "        x = self.data[ind]\n",
        "        y = self.y[ind]\n",
        "        if self.transform:\n",
        "          x = self.transform(x)\n",
        "        return x, y\n",
        "  \n",
        "# This dataloader is to extract features - using toTensor() to swap channels https://stackoverflow.com/questions/64629702/pytorch-transform-totensor-changes-image \n",
        "\n",
        "train_set = TrainDataset_improved(origin_X_train, y_train, train_transform)\n",
        "test_set  = TrainDataset_improved(origin_X_test, y_test,  valid_transform)\n",
        "\n",
        "batch_size = 1\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False) # since is temporal data, do not shuffle on training\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6JyB7qLDc9d",
        "outputId": "3b0170c9-35a8-4fca-c6d9-7931d43a99b3"
      },
      "source": [
        "# Verify that transformation flipped channels order for pre-trained model\n",
        "cont = 0\n",
        "for batch, targets in train_loader:\n",
        "  print(batch.shape)\n",
        "  if cont >3:\n",
        "    break\n",
        "  cont+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 844, 536])\n",
            "torch.Size([1, 3, 844, 536])\n",
            "torch.Size([1, 3, 844, 536])\n",
            "torch.Size([1, 3, 844, 536])\n",
            "torch.Size([1, 3, 844, 536])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjsAmcfq05a-",
        "outputId": "06949e1f-d0a9-4442-eb55-7a87abb6413c"
      },
      "source": [
        "train_loader.dataset.data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(116, 844, 536, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoWwCc2D0cWK",
        "outputId": "d9d1e807-a423-4397-a46d-909a4dd1e832"
      },
      "source": [
        "def extract_features(loader):\n",
        "    \n",
        "    # put the model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    features, labels = [], []\n",
        "    cont = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, targets in loader:\n",
        "            #print(cont)\n",
        "            batch = batch.float()\n",
        "            #print(batch.shape)\n",
        "            # extract the features using the model\n",
        "            batch_features = model(batch.to(device))\n",
        "\n",
        "            features.append(batch_features.data.cpu().numpy())\n",
        "            labels.append(targets.numpy())\n",
        "            cont+=1\n",
        "\n",
        "    features = np.concatenate(features, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(train_loader)\n",
        "valid_features, valid_labels = extract_features(test_loader)\n",
        "\n",
        "print(f'train features are {train_features.shape}')\n",
        "print(f'valid features are {valid_features.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train features are (116, 1000)\n",
            "valid features are (29, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4pWRi0UGUdl"
      },
      "source": [
        "train_set = TrainDataset(train_features, train_labels)\n",
        "test_set  = TrainDataset(valid_features, valid_labels)\n",
        "\n",
        "batch_size = 1\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udL0ELngFsQn",
        "outputId": "5b110b3f-4c79-441f-da14-5d01e1172798"
      },
      "source": [
        "input_size = train_features.shape[1] # i.e. 1000 \n",
        "print(input_size)\n",
        "hidden_size = 1000\n",
        "num_layers = 2\n",
        "sequence_length = 1\n",
        "num_classes = 1\n",
        "output_size = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ9YoYlazlft"
      },
      "source": [
        "class RNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc_1 = nn.Linear(hidden_size,1024)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc_1_1 = nn.Linear(1024,512)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc   = nn.Linear(512, output_size)   \n",
        "        #self.final = torch.log_softmax(output_size, dim=1)\n",
        "        \n",
        "    def forward(self, x, hs, cs):\n",
        "      \n",
        "        out, (hs,cs) = self.lstm(x, (hs,cs)) # out.shape = (batch_size, seq_len, hidden_size)\n",
        "        # output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
        "        out = out.view(-1, self.hidden_size) # out.shape = (seq_len, hidden_size)     \n",
        "        out = self.fc_1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.fc_1_1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc(out)       \n",
        "        return out\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = out.reshape(out.shape[0], -1)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "        \n",
        "# Recurrent neural network with GRU (many-to-one)\n",
        "class RNN_GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN_GRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = out.reshape(out.shape[0], -1)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyrlaDspzlfu",
        "outputId": "6227701d-d0af-4db3-c4ef-9dd97130be55"
      },
      "source": [
        "model = flightLSTM(input_size, hidden_size, num_layers, output_size)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "flightLSTM(\n",
              "  (lstm): LSTM(1000, 1000, num_layers=2, batch_first=True)\n",
              "  (fc_1): Linear(in_features=1000, out_features=128, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pxNUkIezlfu",
        "outputId": "11dd3a5a-0333-47d1-b999-c232fecb97c3"
      },
      "source": [
        "#from model import MPL_model\n",
        "\n",
        "# https://medium.com/deep-learning-study-notes/multi-layer-perceptron-mlp-in-pytorch-21ea46d50e62\n",
        "# cnn https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 100\n",
        "running_mae = 0\n",
        "\n",
        "losses = []\n",
        "\n",
        "v_losses = []\n",
        "# MAE\n",
        "maes = []\n",
        "mae_list = []\n",
        "v_maes = []\n",
        "v_mae_list = []\n",
        "# MAPE\n",
        "mape = []\n",
        "mapes = []\n",
        "mapes_list = []\n",
        "mape_val = []\n",
        "mapes_val = []\n",
        "mapes_val_list = []\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer):\n",
        "\n",
        "    for batch_num, input_data in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        x, y = input_data\n",
        "        x = x.to(device).float()\n",
        "        y = y.to(device)\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        hs = torch.zeros(2, x.size(0), hidden_size).to(device)  \n",
        "        cs = torch.zeros(2, x.size(0), hidden_size).to(device)\n",
        "\n",
        "        output = model(x,hs,cs)\n",
        "        #output = model(x)\n",
        "\n",
        "        loss = criterion(output, y.float())\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE\n",
        "        error = torch.abs(output - y).sum().data\n",
        "        maes.append(error)\n",
        "\n",
        "        # MAPE \n",
        "        mape = torch.abs((output-y)/output).sum().data\n",
        "        mapes.append(mape)\n",
        "    \n",
        "    return maes, mapes\n",
        "\n",
        "def test(train_loader, model, criterion, optimizer):\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_num, input_data in enumerate(test_loader):\n",
        "          model.eval()\n",
        "          optimizer.zero_grad()\n",
        "          x, y = input_data\n",
        "          x = x.to(device).float()\n",
        "          y = y.to(device)\n",
        "          x = x.unsqueeze(0)\n",
        "\n",
        "          hs = torch.zeros(2, x.size(0), hidden_size).to(device)  \n",
        "          cs = torch.zeros(2, x.size(0), hidden_size).to(device)\n",
        "\n",
        "          output = model(x,hs,cs)\n",
        "          #output = model(x)\n",
        "          v_loss = criterion(output, y.float())\n",
        "          \n",
        "          v_losses.append(v_loss.item())\n",
        "          \n",
        "          \n",
        "          # MAE\n",
        "          v_error = torch.abs(output - y).sum().data\n",
        "          v_maes.append(v_error)\n",
        "\n",
        "          # MAPE \n",
        "          \n",
        "          mape_val = torch.abs((output-y)/output).sum().data\n",
        "          mapes_val.append(mape_val)\n",
        "      return v_maes, mapes_val\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "\n",
        "    maes, mapes = train(train_loader, model, criterion, optimizer)\n",
        "\n",
        "    v_maes, mapes_val = test(test_loader, model, criterion, optimizer)\n",
        "    \n",
        "    # MAE\n",
        "\n",
        "    mae_list.append(sum(maes)/len(maes))\n",
        "    v_mae_list.append(sum(v_maes)/len(v_maes))\n",
        "\n",
        "    mapes_list.append(sum(mapes)/len(mapes))\n",
        "    mapes_val_list.append(sum(mapes_val)/len(mapes_val))\n",
        "\n",
        "    print('Epoch %d | Loss_training %6.2f | MAE_training %6.2f | MAPE_training %6.2f' % (epoch, sum(losses)/len(losses), sum(maes)/len(maes), \n",
        "                                                                                         sum(mapes)/len(mapes)))\n",
        "    print('Epoch %d | val_Loss %6.2f | val_MAE %6.2f | MAPE_val %6.2f' % (epoch, sum(v_losses)/len(v_losses), sum(v_maes)/len(v_maes),\n",
        "                                                                                         sum(mapes_val)/len(mapes_val)))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning:\n",
            "\n",
            "Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss_training 32897.84 | MAE_training 140.35 | MAPE_training   1.19\n",
            "Epoch 0 | val_Loss  37.15 | val_MAE   4.43 | MAPE_val   0.21\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Epoch 1 | Loss_training 35991.15 | MAE_training 134.39 | MAPE_training   1.88\n",
            "Epoch 1 | val_Loss  40.45 | val_MAE   4.71 | MAPE_val   0.23\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Epoch 2 | Loss_training 37168.69 | MAE_training 137.36 | MAPE_training   7.33\n",
            "Epoch 2 | val_Loss  91.17 | val_MAE   7.25 | MAPE_val   0.27\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Epoch 3 | Loss_training 37395.91 | MAE_training 138.16 | MAPE_training   6.05\n",
            "Epoch 3 | val_Loss 226.16 | val_MAE  11.13 | MAPE_val   0.33\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Epoch 4 | Loss_training 37059.73 | MAE_training 139.60 | MAPE_training   5.17\n",
            "Epoch 4 | val_Loss 817.45 | val_MAE  19.91 | MAPE_val   0.40\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Epoch 5 | Loss_training 36352.37 | MAE_training 140.54 | MAPE_training   4.52\n",
            "Epoch 5 | val_Loss 1411.01 | val_MAE  27.23 | MAPE_val   0.46\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Epoch 6 | Loss_training 35822.08 | MAE_training 141.56 | MAPE_training   4.05\n",
            "Epoch 6 | val_Loss 1976.37 | val_MAE  33.35 | MAPE_val   0.50\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Epoch 7 | Loss_training 35363.18 | MAE_training 142.22 | MAPE_training   3.69\n",
            "Epoch 7 | val_Loss 2391.38 | val_MAE  37.95 | MAPE_val   0.53\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Epoch 8 | Loss_training 35006.67 | MAE_training 142.74 | MAPE_training   3.41\n",
            "Epoch 8 | val_Loss 2730.69 | val_MAE  41.56 | MAPE_val   0.55\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Epoch 9 | Loss_training 34706.94 | MAE_training 143.13 | MAPE_training   3.19\n",
            "Epoch 9 | val_Loss 3024.34 | val_MAE  44.63 | MAPE_val   0.57\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Epoch 10 | Loss_training 34454.53 | MAE_training 143.45 | MAPE_training   3.01\n",
            "Epoch 10 | val_Loss 3273.70 | val_MAE  47.19 | MAPE_val   0.59\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Epoch 11 | Loss_training 34224.03 | MAE_training 143.59 | MAPE_training   2.85\n",
            "Epoch 11 | val_Loss 3481.11 | val_MAE  49.31 | MAPE_val   0.61\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Epoch 12 | Loss_training 34017.94 | MAE_training 143.68 | MAPE_training   2.72\n",
            "Epoch 12 | val_Loss 3673.64 | val_MAE  51.16 | MAPE_val   0.62\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Epoch 13 | Loss_training 33812.08 | MAE_training 143.52 | MAPE_training   2.61\n",
            "Epoch 13 | val_Loss 3601.86 | val_MAE  50.70 | MAPE_val   0.62\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Epoch 14 | Loss_training 33799.41 | MAE_training 143.23 | MAPE_training   2.53\n",
            "Epoch 14 | val_Loss 3724.45 | val_MAE  51.75 | MAPE_val   0.63\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Epoch 15 | Loss_training 33675.47 | MAE_training 143.16 | MAPE_training   2.45\n",
            "Epoch 15 | val_Loss 3903.69 | val_MAE  53.20 | MAPE_val   0.63\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Epoch 16 | Loss_training 33520.27 | MAE_training 143.11 | MAPE_training   2.38\n",
            "Epoch 16 | val_Loss 4090.63 | val_MAE  54.66 | MAPE_val   0.64\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Epoch 17 | Loss_training 33373.73 | MAE_training 143.09 | MAPE_training   2.31\n",
            "Epoch 17 | val_Loss 4261.79 | val_MAE  55.98 | MAPE_val   0.65\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Epoch 18 | Loss_training 33240.48 | MAE_training 143.07 | MAPE_training   2.25\n",
            "Epoch 18 | val_Loss 4445.98 | val_MAE  57.27 | MAPE_val   0.66\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Epoch 19 | Loss_training 33066.55 | MAE_training 142.76 | MAPE_training   2.19\n",
            "Epoch 19 | val_Loss 4423.80 | val_MAE  57.01 | MAPE_val   0.66\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Epoch 20 | Loss_training 33022.08 | MAE_training 142.61 | MAPE_training   2.15\n",
            "Epoch 20 | val_Loss 4438.69 | val_MAE  57.03 | MAPE_val   0.66\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Epoch 21 | Loss_training 32985.57 | MAE_training 142.67 | MAPE_training   2.11\n",
            "Epoch 21 | val_Loss 4537.34 | val_MAE  57.69 | MAPE_val   0.66\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Epoch 22 | Loss_training 32908.43 | MAE_training 142.69 | MAPE_training   2.07\n",
            "Epoch 22 | val_Loss 4676.12 | val_MAE  58.57 | MAPE_val   0.67\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Epoch 23 | Loss_training 32811.60 | MAE_training 142.67 | MAPE_training   2.03\n",
            "Epoch 23 | val_Loss 4815.11 | val_MAE  59.45 | MAPE_val   0.67\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Epoch 24 | Loss_training 32716.53 | MAE_training 142.62 | MAPE_training   2.00\n",
            "Epoch 24 | val_Loss 4968.50 | val_MAE  60.39 | MAPE_val   0.67\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Epoch 25 | Loss_training 32616.02 | MAE_training 142.53 | MAPE_training   1.96\n",
            "Epoch 25 | val_Loss 5120.60 | val_MAE  61.30 | MAPE_val   0.68\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Epoch 26 | Loss_training 32512.82 | MAE_training 142.41 | MAPE_training   1.93\n",
            "Epoch 26 | val_Loss 5246.25 | val_MAE  62.10 | MAPE_val   0.68\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Epoch 27 | Loss_training 32419.68 | MAE_training 142.30 | MAPE_training   1.90\n",
            "Epoch 27 | val_Loss 5366.18 | val_MAE  62.83 | MAPE_val   0.68\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Epoch 28 | Loss_training 32333.95 | MAE_training 142.19 | MAPE_training   1.88\n",
            "Epoch 28 | val_Loss 5474.66 | val_MAE  63.54 | MAPE_val   0.69\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Epoch 29 | Loss_training 32247.33 | MAE_training 142.07 | MAPE_training   1.85\n",
            "Epoch 29 | val_Loss 5534.75 | val_MAE  63.98 | MAPE_val   0.69\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Epoch 30 | Loss_training 32182.92 | MAE_training 141.97 | MAPE_training   1.83\n",
            "Epoch 30 | val_Loss 5636.44 | val_MAE  64.63 | MAPE_val   0.69\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Epoch 31 | Loss_training 32098.10 | MAE_training 141.83 | MAPE_training   1.81\n",
            "Epoch 31 | val_Loss 5688.75 | val_MAE  65.05 | MAPE_val   0.70\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Epoch 32 | Loss_training 32028.58 | MAE_training 141.74 | MAPE_training   1.79\n",
            "Epoch 32 | val_Loss 5778.05 | val_MAE  65.60 | MAPE_val   0.70\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Epoch 33 | Loss_training 31950.91 | MAE_training 141.59 | MAPE_training   1.77\n",
            "Epoch 33 | val_Loss 5682.68 | val_MAE  64.73 | MAPE_val   0.69\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Epoch 34 | Loss_training 31955.63 | MAE_training 141.32 | MAPE_training   1.76\n",
            "Epoch 34 | val_Loss 5689.97 | val_MAE  64.74 | MAPE_val   0.69\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Epoch 35 | Loss_training 31916.16 | MAE_training 141.18 | MAPE_training   1.74\n",
            "Epoch 35 | val_Loss 5715.33 | val_MAE  64.98 | MAPE_val   0.70\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Epoch 36 | Loss_training 31846.94 | MAE_training 140.96 | MAPE_training   1.73\n",
            "Epoch 36 | val_Loss 5662.69 | val_MAE  64.33 | MAPE_val   0.69\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Epoch 37 | Loss_training 31843.34 | MAE_training 140.71 | MAPE_training   1.72\n",
            "Epoch 37 | val_Loss 5650.73 | val_MAE  64.09 | MAPE_val   0.69\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Epoch 38 | Loss_training 31831.01 | MAE_training 140.65 | MAPE_training   1.71\n",
            "Epoch 38 | val_Loss 5688.74 | val_MAE  64.32 | MAPE_val   0.69\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Epoch 39 | Loss_training 31763.76 | MAE_training 140.41 | MAPE_training   1.70\n",
            "Epoch 39 | val_Loss 5637.02 | val_MAE  63.88 | MAPE_val   0.69\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Epoch 40 | Loss_training 31747.07 | MAE_training 140.27 | MAPE_training   1.69\n",
            "Epoch 40 | val_Loss 5586.36 | val_MAE  63.47 | MAPE_val   0.69\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Epoch 41 | Loss_training 31715.84 | MAE_training 139.89 | MAPE_training   1.67\n",
            "Epoch 41 | val_Loss 5512.47 | val_MAE  62.31 | MAPE_val   0.68\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Epoch 42 | Loss_training 31717.46 | MAE_training 139.19 | MAPE_training   1.68\n",
            "Epoch 42 | val_Loss 5388.13 | val_MAE  61.02 | MAPE_val   0.67\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Epoch 43 | Loss_training 31749.81 | MAE_training 139.16 | MAPE_training   1.68\n",
            "Epoch 43 | val_Loss 5266.81 | val_MAE  59.76 | MAPE_val   0.66\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Epoch 44 | Loss_training 31855.47 | MAE_training 139.25 | MAPE_training   1.68\n",
            "Epoch 44 | val_Loss 5154.65 | val_MAE  58.74 | MAPE_val   0.66\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Epoch 45 | Loss_training 31939.39 | MAE_training 139.52 | MAPE_training   1.68\n",
            "Epoch 45 | val_Loss 5101.58 | val_MAE  58.52 | MAPE_val   0.66\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Epoch 46 | Loss_training 31946.54 | MAE_training 139.58 | MAPE_training   1.67\n",
            "Epoch 46 | val_Loss 5050.45 | val_MAE  58.25 | MAPE_val   0.66\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Epoch 47 | Loss_training 31974.33 | MAE_training 139.71 | MAPE_training   1.67\n",
            "Epoch 47 | val_Loss 5037.07 | val_MAE  58.29 | MAPE_val   0.66\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Epoch 48 | Loss_training 31975.31 | MAE_training 139.81 | MAPE_training   1.66\n",
            "Epoch 48 | val_Loss 5036.88 | val_MAE  58.40 | MAPE_val   0.66\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Epoch 49 | Loss_training 31963.29 | MAE_training 139.88 | MAPE_training   1.65\n",
            "Epoch 49 | val_Loss 5030.22 | val_MAE  58.49 | MAPE_val   0.66\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Epoch 50 | Loss_training 31954.20 | MAE_training 139.95 | MAPE_training   1.64\n",
            "Epoch 50 | val_Loss 5031.98 | val_MAE  58.62 | MAPE_val   0.66\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Epoch 51 | Loss_training 31928.02 | MAE_training 139.94 | MAPE_training   1.63\n",
            "Epoch 51 | val_Loss 5014.90 | val_MAE  58.62 | MAPE_val   0.66\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Epoch 52 | Loss_training 31908.96 | MAE_training 139.93 | MAPE_training   1.62\n",
            "Epoch 52 | val_Loss 4987.79 | val_MAE  58.52 | MAPE_val   0.66\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Epoch 53 | Loss_training 31903.92 | MAE_training 139.96 | MAPE_training   1.62\n",
            "Epoch 53 | val_Loss 4984.58 | val_MAE  58.59 | MAPE_val   0.67\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Epoch 54 | Loss_training 31886.13 | MAE_training 139.96 | MAPE_training   1.61\n",
            "Epoch 54 | val_Loss 4987.95 | val_MAE  58.68 | MAPE_val   0.67\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Epoch 55 | Loss_training 31856.34 | MAE_training 139.89 | MAPE_training   1.60\n",
            "Epoch 55 | val_Loss 4959.74 | val_MAE  58.55 | MAPE_val   0.67\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Epoch 56 | Loss_training 31843.84 | MAE_training 139.89 | MAPE_training   1.60\n",
            "Epoch 56 | val_Loss 4927.43 | val_MAE  58.39 | MAPE_val   0.67\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Epoch 57 | Loss_training 31832.65 | MAE_training 139.83 | MAPE_training   1.59\n",
            "Epoch 57 | val_Loss 4894.68 | val_MAE  58.20 | MAPE_val   0.67\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Epoch 58 | Loss_training 31816.50 | MAE_training 139.67 | MAPE_training   1.58\n",
            "Epoch 58 | val_Loss 4827.64 | val_MAE  57.41 | MAPE_val   0.66\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Epoch 59 | Loss_training 31856.41 | MAE_training 139.58 | MAPE_training   1.59\n",
            "Epoch 59 | val_Loss 4793.79 | val_MAE  57.08 | MAPE_val   0.66\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Epoch 60 | Loss_training 31846.15 | MAE_training 139.43 | MAPE_training   1.59\n",
            "Epoch 60 | val_Loss 4731.02 | val_MAE  56.48 | MAPE_val   0.65\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Epoch 61 | Loss_training 31858.64 | MAE_training 139.37 | MAPE_training   1.59\n",
            "Epoch 61 | val_Loss 4701.16 | val_MAE  56.20 | MAPE_val   0.65\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Epoch 62 | Loss_training 31854.08 | MAE_training 139.33 | MAPE_training   1.58\n",
            "Epoch 62 | val_Loss 4670.29 | val_MAE  55.93 | MAPE_val   0.65\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Epoch 63 | Loss_training 31838.12 | MAE_training 139.18 | MAPE_training   1.58\n",
            "Epoch 63 | val_Loss 4613.58 | val_MAE  55.23 | MAPE_val   0.65\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Epoch 64 | Loss_training 31870.88 | MAE_training 139.08 | MAPE_training   1.59\n",
            "Epoch 64 | val_Loss 4609.44 | val_MAE  55.20 | MAPE_val   0.65\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Epoch 65 | Loss_training 31831.71 | MAE_training 138.87 | MAPE_training   1.58\n",
            "Epoch 65 | val_Loss 4545.39 | val_MAE  54.56 | MAPE_val   0.64\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Epoch 66 | Loss_training 31845.28 | MAE_training 138.85 | MAPE_training   1.58\n",
            "Epoch 66 | val_Loss 4500.63 | val_MAE  54.04 | MAPE_val   0.64\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Epoch 67 | Loss_training 31840.12 | MAE_training 138.66 | MAPE_training   1.58\n",
            "Epoch 67 | val_Loss 4445.69 | val_MAE  53.53 | MAPE_val   0.64\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Epoch 68 | Loss_training 31842.97 | MAE_training 138.60 | MAPE_training   1.58\n",
            "Epoch 68 | val_Loss 4393.86 | val_MAE  53.04 | MAPE_val   0.63\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Epoch 69 | Loss_training 31851.10 | MAE_training 138.58 | MAPE_training   1.58\n",
            "Epoch 69 | val_Loss 4345.29 | val_MAE  52.66 | MAPE_val   0.63\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Epoch 70 | Loss_training 31836.30 | MAE_training 138.41 | MAPE_training   1.58\n",
            "Epoch 70 | val_Loss 4298.87 | val_MAE  52.08 | MAPE_val   0.63\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Epoch 71 | Loss_training 31833.02 | MAE_training 138.23 | MAPE_training   1.57\n",
            "Epoch 71 | val_Loss 4240.91 | val_MAE  51.44 | MAPE_val   0.62\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Epoch 72 | Loss_training 31853.25 | MAE_training 138.19 | MAPE_training   1.58\n",
            "Epoch 72 | val_Loss 4197.55 | val_MAE  50.92 | MAPE_val   0.62\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Epoch 73 | Loss_training 31858.54 | MAE_training 138.20 | MAPE_training   1.58\n",
            "Epoch 73 | val_Loss 4161.85 | val_MAE  50.69 | MAPE_val   0.62\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Epoch 74 | Loss_training 31857.18 | MAE_training 138.14 | MAPE_training   1.57\n",
            "Epoch 74 | val_Loss 4128.70 | val_MAE  50.32 | MAPE_val   0.61\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Epoch 75 | Loss_training 31856.25 | MAE_training 138.09 | MAPE_training   1.57\n",
            "Epoch 75 | val_Loss 4091.87 | val_MAE  50.04 | MAPE_val   0.61\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Epoch 76 | Loss_training 31838.50 | MAE_training 137.92 | MAPE_training   1.57\n",
            "Epoch 76 | val_Loss 4047.19 | val_MAE  49.58 | MAPE_val   0.61\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Epoch 77 | Loss_training 31828.81 | MAE_training 137.83 | MAPE_training   1.57\n",
            "Epoch 77 | val_Loss 4010.35 | val_MAE  49.25 | MAPE_val   0.61\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Epoch 78 | Loss_training 31814.88 | MAE_training 137.79 | MAPE_training   1.56\n",
            "Epoch 78 | val_Loss 3988.93 | val_MAE  49.15 | MAPE_val   0.61\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Epoch 79 | Loss_training 31788.26 | MAE_training 137.66 | MAPE_training   1.56\n",
            "Epoch 79 | val_Loss 3943.32 | val_MAE  48.69 | MAPE_val   0.61\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Epoch 80 | Loss_training 31784.40 | MAE_training 137.58 | MAPE_training   1.56\n",
            "Epoch 80 | val_Loss 3913.47 | val_MAE  48.52 | MAPE_val   0.61\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Epoch 81 | Loss_training 31757.90 | MAE_training 137.44 | MAPE_training   1.55\n",
            "Epoch 81 | val_Loss 3876.22 | val_MAE  48.20 | MAPE_val   0.60\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Epoch 82 | Loss_training 31734.97 | MAE_training 137.30 | MAPE_training   1.55\n",
            "Epoch 82 | val_Loss 3842.69 | val_MAE  47.86 | MAPE_val   0.60\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Epoch 83 | Loss_training 31709.87 | MAE_training 137.14 | MAPE_training   1.55\n",
            "Epoch 83 | val_Loss 3805.98 | val_MAE  47.52 | MAPE_val   0.60\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Epoch 84 | Loss_training 31684.52 | MAE_training 136.99 | MAPE_training   1.54\n",
            "Epoch 84 | val_Loss 3764.20 | val_MAE  47.10 | MAPE_val   0.60\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Epoch 85 | Loss_training 31671.41 | MAE_training 136.83 | MAPE_training   1.54\n",
            "Epoch 85 | val_Loss 3730.72 | val_MAE  46.81 | MAPE_val   0.60\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Epoch 86 | Loss_training 31634.97 | MAE_training 136.66 | MAPE_training   1.54\n",
            "Epoch 86 | val_Loss 3690.76 | val_MAE  46.40 | MAPE_val   0.60\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Epoch 87 | Loss_training 31608.53 | MAE_training 136.50 | MAPE_training   1.54\n",
            "Epoch 87 | val_Loss 3653.49 | val_MAE  46.05 | MAPE_val   0.60\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Epoch 88 | Loss_training 31580.75 | MAE_training 136.35 | MAPE_training   1.53\n",
            "Epoch 88 | val_Loss 3616.08 | val_MAE  45.67 | MAPE_val   0.59\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Epoch 89 | Loss_training 31556.41 | MAE_training 136.22 | MAPE_training   1.53\n",
            "Epoch 89 | val_Loss 3588.47 | val_MAE  45.43 | MAPE_val   0.59\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Epoch 90 | Loss_training 31516.86 | MAE_training 136.07 | MAPE_training   1.53\n",
            "Epoch 90 | val_Loss 3560.15 | val_MAE  45.19 | MAPE_val   0.59\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Epoch 91 | Loss_training 31479.96 | MAE_training 135.93 | MAPE_training   1.52\n",
            "Epoch 91 | val_Loss 3535.97 | val_MAE  44.99 | MAPE_val   0.59\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Epoch 92 | Loss_training 31436.91 | MAE_training 135.76 | MAPE_training   1.52\n",
            "Epoch 92 | val_Loss 3504.83 | val_MAE  44.70 | MAPE_val   0.59\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Epoch 93 | Loss_training 31397.70 | MAE_training 135.60 | MAPE_training   1.51\n",
            "Epoch 93 | val_Loss 3480.41 | val_MAE  44.50 | MAPE_val   0.59\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Epoch 94 | Loss_training 31358.75 | MAE_training 135.47 | MAPE_training   1.51\n",
            "Epoch 94 | val_Loss 3459.26 | val_MAE  44.35 | MAPE_val   0.59\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Epoch 95 | Loss_training 31317.97 | MAE_training 135.32 | MAPE_training   1.51\n",
            "Epoch 95 | val_Loss 3440.05 | val_MAE  44.18 | MAPE_val   0.59\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Epoch 96 | Loss_training 31268.49 | MAE_training 135.15 | MAPE_training   1.50\n",
            "Epoch 96 | val_Loss 3419.94 | val_MAE  43.99 | MAPE_val   0.58\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Epoch 97 | Loss_training 31221.46 | MAE_training 134.98 | MAPE_training   1.50\n",
            "Epoch 97 | val_Loss 3396.29 | val_MAE  43.76 | MAPE_val   0.58\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Epoch 98 | Loss_training 31191.43 | MAE_training 134.84 | MAPE_training   1.49\n",
            "Epoch 98 | val_Loss 3368.35 | val_MAE  43.50 | MAPE_val   0.58\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Epoch 99 | Loss_training 31150.42 | MAE_training 134.63 | MAPE_training   1.49\n",
            "Epoch 99 | val_Loss 3340.57 | val_MAE  43.24 | MAPE_val   0.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7n49G3ezlfu",
        "outputId": "67d7136f-0bd6-41e5-866e-9cd186d00675"
      },
      "source": [
        "epochs = range(0,100)\n",
        "plt.figure(figsize=[8., 6.])\n",
        "plt.plot(epochs, mae_list,  label='MAE')\n",
        "plt.plot(epochs, v_mae_list, label='Val_MAE')\n",
        "plt.title('Training MAE vs epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad-Uj4Un0Y0p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}